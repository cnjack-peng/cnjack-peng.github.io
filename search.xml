<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Percona XtraBackup]]></title>
    <url>%2F2017%2F12%2F25%2Fpercona-xtrabackup%2F</url>
    <content type="text"><![CDATA[Percona XtraBackup是一个相对完美的免费开源数据备份工具，支持在线无锁表同步复制和可并行高效率的安全备份恢复。 备份原理Percona XtraBackup 基于InnoDB引擎的故障恢复功能。它拷贝InnoDB的数据文件，但是数据文件内存的数据并不是一致的；但是通过 XtraBackup 执行故障恢复确保了数据的一致性。Percona XtraBackup 保证数据一致性 是通过InnoDB的redo log（也叫事务日志 transaction log）。redo log记录了InnoDB 数据的每次改变。当InnoDB（MySQL）启动，它检查数据文件和事务日志（redo / transaction log），然后执行如下两步: 应用已经提交的事务 回滚已经修改但是没有提交的事务 Percona XtraBackup 备份过程如下： 备份InnoDB引擎：当 Percona XtraBackup 开始工作，它记录所有 LSN （ log sequence number ），然后拷贝所有数据文件。拷贝数据文件的同时， Percona XtraBackup 的一个后台进程监测 事务日志（redo / transaction log），然后从事务日志中拷贝改变的部分。由于事务日志（log/transaction log）写日志采取round-robin方式，短时间事务日志就能够被重用，所以Percona XtraBackup 必须一致保持监测事务日志并拷贝改变部分。Percona XtraBackup 从执行开始，就必须记录每一条事务日志，不然无法保证一致性和完整性。 备份其他存储引擎（如MyISAM）：Percona XtraBackup 备份 InnoDB/XtraDB 数据文件和事务日志完毕后。使用 FLUSH TABLES WITH READ LOCK 锁，阻止对MyISAM的DML操作,（从MySQL 5.6开始），然后备份拷贝MyISAM等non-InnoDB的相关文件，如.frm, .MRG, .MYD, .MYI,.TRG, .TRN, .ARM, .ARZ, .CSM, .CSV, .par, 和 .opt 文件。 注意，备份MyISAM时，执行FLUSH TABLES WITH READ LOCK ，并不会影响InnoDB。. 安装与使用官方网站：https://www.percona.com/downloads/XtraBackup/LATEST/，下载适合你的平台软件，然后安装，需要注意的是如果下载的是RPM包或者源码，需要手动解决依赖的相关问题(报错:Transaction Check Error:)，推荐使用Percona仓库安装：12345yum install http://www.percona.com/downloads/percona-release/redhat/0.1-4/percona-release-0.1-4.noarch.rpmyum list | grep perconayum install percona-xtrabackup-24# 用于解压备份文件yum install qpress 全量备份：123456# 备份并压缩xtrabackup --backup --compress --target-dir=/backup_store/db_back/# 解压，必须解压才能--preparextrabackup --backup --decompress --target-dir=/backup_store/db_back/# prepare之前，数据文件是不一致的，因为它们在不同时间点被备份因此，--prepare会使所有数据文件的步调达成一致xtrabackup --prepare --target-dir=/backup_store/db_back/ 增量备份：12 基于XtraBackup搭建主从12345678910111213141516171819202122232425262728293031323334353637383940414243#原有主数据库版本mysql -Vmysql Ver 14.14 Distrib 5.5.31, for Linux (x86_64) using readline 5.1#迁移从数据库版本mysql -Vmysql Ver 14.14 Distrib 5.6.25, for linux-glibc2.5 (x86_64) using EditLine wrapper#检查数据库引擎show engines;+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || CSV | YES | CSV storage engine | NO | NO | NO || MyISAM | YES | MyISAM storage engine | NO | NO | NO || BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO || MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES || ARCHIVE | YES | Archive storage engine | NO | NO | NO || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+#主从数据库同步注意点[mysqld]#主从之间的id不能相同server-id#启用二进制日志log-bin#一般在从库开启（可选）read_only#推荐使用InnoDB并做好相关配置#检查主从数据库状态mysql -S /tmp/mysql.sock -e "show global variables like 'server_id';"+---------------+-------+| Variable_name | Value |+---------------+-------+| server_id | 1 |+---------------+-------+mysql -S /tmp/mysql.sock -e "show global variables like 'log_bin';"+---------------+-------+| Variable_name | Value |+---------------+-------+| log_bin | ON |+---------------+-------+ 备份和恢复通常一般都直接使用innobackupex，因为它能同时备份InnoDB和MyISAM引擎的表。重点关注Slave_IO_Running和Slave_SQL_Runningd的状态是否为YES。12345678910111213141516171819202122232425262728293031323334353637383940414243#备份innobackupex --socket=/usr/local/var/mysql2/mysql2.sock --user=root --password --defaults-file=/etc/mysqld_multi.cnf --parallel=4 --database=passport /tmp/backup#保持事务一致性innobackupex --socket=/usr/local/var/mysql2/mysql2.sock --user=root --password --defaults-file=/etc/mysqld_multi.cnf --database=passport --apply-log /tmp/backup/2015-08-05_16-08-14#传输scp -r /tmp/backup/2015-08-05_16-08-14 10.10.16.24:/tmp/backup/ #恢复innobackupex --socket=/tmp/mysql.sock --user=root --password --defaults-file=/app/local/mysql/my.cnf --copy-back /tmp/backup/2015-08-05_16-08-14/#还原权限chown -R mysql:mysql /app/data/mysql/dataservice mysqld start/app/local/mysql/scripts/mysql_install_db --basedir=/app/local/mysql --datadir=/app/data/mysql/data --no-defaults --skip-name-resolve --user=mysql#主库授权同步帐号SELECT DISTINCT CONCAT('User: ''',user,'''@''',host,''';') AS query FROM mysql.user;GRANT REPLICATION SLAVE ON *.* TO 'slave_passport'@'10.10.16.24' IDENTIFIED BY 'slave_passport';FLUSH PRIVILEGES;#从库开启同步cat /tmp/backup/2015-08-05_16-08-14/xtrabackup_binlog_info mysql-bin.002599 804497686CHANGE MASTER TOMASTER_HOST='10.10.16.51',MASTER_USER='slave_passport',MASTER_PASSWORD='slave_passport',MASTER_PORT=3307,MASTER_LOG_FILE='mysql-bin.002599',MASTER_LOG_POS=804497686;#开启主从同步start slave;#查看从库状态show slave status\ G#从库的检查参数Slave_IO_Running=YesSlave_SQL_Running=Yes#主库的检查参数show master status \G+------------------+-----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+-----------+--------------+------------------+| mysql-bin.002600 | 454769337 | | |+------------------+-----------+--------------+------------------+1 row in set (0.00 sec)show processlist;Master has sent all binlog to slave; waiting for binlog to be updated MySQL主从切换切换前断开主库访问连接观察进程状态，无写操作后再停止从库IO_THREAD进行切换。1234567891011121314151617181920212223242526272829303132333435#查看主库状态show processlist;Master has sent all binlog to slave; waiting for binlog to be updatedshow master status \G#从库停止 IO_THREAD 线程stop slave IO_THREAD;show processlist;Slave has read all relay log; waiting for the slave I/O thread to update itshow slave status \G#从库切换为主库stop slave;reset master;reset slave all;show master status \G#激活帐户SELECT DISTINCT CONCAT('User: ''',user,'''@''',host,''';') AS query FROM mysql.user;GRANT REPLICATION SLAVE ON *.* TO 'slave_passport'@'10.10.16.51' IDENTIFIED BY 'slave_passport';FLUSH PRIVILEGES;#切换原有主库为从库reset master;reset slave all;CHANGE MASTER TOMASTER_HOST='10.10.16.24',MASTER_USER='slave_passport',MASTER_PASSWORD='slave_passport',MASTER_PORT=3306,MASTER_LOG_FILE='mysql-bin.000001',MASTER_LOG_POS=804497686;#检查主库SHOW PROCESSLIST;show master status \G#启动从库SHOW PROCESSLIST;start slave;show slave status \G 常见问题Slave_SQL_Running:No1234#一般是事务回滚造成的stop slave;set GLOBAL SQL_SLAVE_SKIP_COUNTER=1;start slave;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Finding a needle in Haystack-Facebook’s photo storage]]></title>
    <url>%2F2017%2F12%2F19%2Ffacebook-haystack%2F</url>
    <content type="text"><![CDATA[面对海量小文件的存储和检索，Google发表了GFS，淘宝开源了TFS，而Facebook又是如何应对千亿级别的图片存储、每秒百万级别的图片查询？Facebook与同样提供了海量图片服务的淘宝，解决方案有何异同？本篇文章，为您揭晓。 本篇论文的原文可谓通俗易懂、行云流水、结构清晰、图文并茂……正如作者所说的——“替换Facebook的图片存储系统就像高速公路上给汽车换轮子，我们无法去追求完美的设计……我们花费了很多的注意力来保持它的简单”，本篇论文也是一样，没有牵扯空洞的庞大架构、也没有晦涩零散的陈述，有的是对痛点的反思，对目标的分解，条理清晰，按部就班。既描述了宏观的整体流程，又推导了细节难点的技术突破过程。以至于译者都不需要在文中插入过多备注和解读了^_^。不过在文章末尾，译者以淘宝的解决方案作为对比，阐述了文章中的一些精髓的突破点，以供读者参考。 摘要本篇论文描述了Haystack，一个为Facebook的照片应用而专门优化定制的对象存储系统。Facebook当前存储了超过260 billion的图片，相当于20PB的数据。用户每个星期还会上传1 billion的新照片（60TB），Facebook在峰值时需提供每秒查询超过1 million图片的能力。相比我们以前的方案(基于NAS和NFS)，Haystack提供了一个成本更低的、性能更高的解决方案。我们观察到一个非常关键的问题：传统的设计因为元数据查询而导致了过多的磁盘操作。我们竭尽全力的减少每个图片的元数据，让Haystack能在内存中执行所有的元数据查询。这个突破让系统腾出了更多的性能来读取真实的数据，增加了整体的吞吐量。 介绍分享照片是Facebook最受欢迎的功能之一。迄今为止，用户已经上传了超过65 billion的图片，使得Facebook成为世界上最大的图片分享网站。对每个上传的照片，Facebook生成和存储4种不同大小的图片（比如在某些场景下只需展示缩略图），这就产生了超过260 billion张图片、超过20PB的数据。用户每个星期还在上传1 billion的新照片（60TB），Facebook峰值时需要提供每秒查询1 million张图片的能力。这些数字未来还会不断增长，图片存储对Facebook的基础设施提出了一个巨大的挑战。 这篇论文介绍了Haystack的设计和实现，它已作为Facebook的图片存储系统投入生产环境24个月了。Haystack是一个为Facebook上分享照片而设计的对象存储技术，在这个应用场景中，每个数据只会写入一次、读操作频繁、从不修改、很少删除。在Facebook遭遇的负荷下，传统的文件系统性能很差，优化定制出Haystack是大势所趋。 根据我们的经验，传统基于POSIX的文件系统的缺点主要是目录和每个文件的元数据。对于图片应用，很多元数据（比如文件权限），是无用的而且浪费了很多存储容量。而且更大的性能消耗在于文件的元数据必须从磁盘读到内存来定位文件。文件规模较小时这些花费无关紧要，然而面对几百billion的图片和PB级别的数据，访问元数据就是吞吐量瓶颈所在。这是我们从之前（NAS+NFS）方案中总结的血的教训。通常情况下，我们读取单个照片就需要好几个磁盘操作：一个（有时候更多）转换文件名为inode number，另一个从磁盘上读取inode，最后一个读取文件本身。简单来说，为了查询元数据使用磁盘I/O是限制吞吐量的重要因素。在实际生产环境中，我们必须依赖内容分发网络（CDN，比如Akamai）来支撑主要的读取流量，即使如此，文件元数据的大小和I/O同样对整体系统有很大影响。 了解传统途径的缺点后，我们设计了Haystack来达到4个主要目标： 高吞吐量和低延迟。我们的图片存储系统必须跟得上海量用户查询请求。超过处理容量上限的请求，要么被忽略（对用户体验是不可接受的），要么被CDN处理（成本昂贵而且可能遭遇一个性价比转折点）。想要用户体验好，图片查询必须快速。Haystack希望每个读操作至多需要一个磁盘操作，基于此才能达到高吞吐量和低延迟。为了实现这个目标，我们竭尽全力的减少每个图片的必需元数据，然后将所有的元数据保存在内存中。 容错。在大规模系统中，故障每天都会发生。尽管服务器崩溃和硬盘故障是不可避免的，也绝不可以给用户返回一个error，哪怕整个数据中心都停电，哪怕一个跨国网络断开。所以，Haystack复制每张图片到地理隔离的多个地点，一台机器倒下了，多台机器会替补上来。 高性价比。Haystack比我们之前（NAS+NFS）方案性能更好，而且更省钱。我们按两个维度来衡量：每TB可用存储的花费、每TB可用存储的读取速度。相对NAS设备，Haystack每个可用TB省了28%的成本，每秒支撑了超过4倍的读请求。 简单。替换Facebook的图片存储系统就像高速公路上给汽车换轮子，我们无法去追求完美的设计，这会导致实现和维护都非常耗时耗力。Haystack是一个新系统，缺乏多年的生产环境级别的测试。我们花费了很多的注意力来保持它的简单，所以构建和部署一个可工作的Haystack只花了几个月而不是好几年。 本篇文章3个主要的贡献是： Haystack，一个为高效存储和检索billion级别图片而优化定制的对象存储系统。 构建和扩展一个低成本、高可靠、高可用图片存储系统中的经验教训。 访问Facebook照片分享应用的请求的特征描述 背景 &amp; 我的前任在本章节，我们将描述Haystack之前的架构，突出其主要的经验教训。由于文章大小限制，一些细节就不细述了。 背景 我们先来看一个概览图，它描述了通常的设计方案，web服务器、CDN和存储系统如何交互协作，来实现一个热门站点的图片服务。图1描述了从用户访问包含某个图片的页面开始，直到她最终从磁盘的特定位置下载此图片结束的全过程。访问一个页面时，用户的浏览器首先发送HTTP请求到一个web服务器，它负责生成markup以供浏览器渲染。对每张图片，web服务器为其构造一个URL，引导浏览器在此位置下载图片数据。对于热门站点，这个URL通常指向一个CDN。如果CDN缓存了此图片，那么它会立刻将数据回复给浏览器。否则，CDN检查URL，URL中需要嵌入足够的信息以供CDN从本站点的存储系统中检索图片。拿到图片后，CDN更新它的缓存数据、将图片发送回用户的浏览器。 基于NFS的设计在我们最初的设计中，我们使用了一个基于NFS的方案。我们吸取的主要教训是，对于一个热门的社交网络站点，只有CDN不足以为图片服务提供一个实用的解决方案。对于热门图片，CDN确实很高效——比如个人信息图片和最近上传的照片——但是一个像Facebook的社交网络站点，会产生大量的对不热门（较老）内容的请求，我们称之为long tail（长尾理论中的名词）。long tail的请求也占据了很大流量，它们都需要访问更下游的图片存储主机，因为这些请求在CDN缓存里基本上都会命中失败。缓存所有的图片是可以解决此问题，但这么做代价太大，需要极大容量的缓存。 基于NFS的设计中，图片文件存储在一组商用NAS设备上，NAS设备的卷被mount到Photo Store Server的NFS上。图2展示了这个架构。Photo Store Server解析URL得出卷和完整的文件路径，在NFS上读取数据，然后返回结果到CDN。 我们最初在NFS卷的每个目录下存储几千个文件，导致读取文件时产生了过多的磁盘操作，哪怕只是读单个图片。由于NAS设备管理目录元数据的机制，放置几千个文件在一个目录是极其低效的，因为目录的blockmap太大不能被设备有效的缓存。因此检索单个图片都可能需要超过10个磁盘操作。在减少到每个目录下几百个图片后，系统仍然大概需要3个磁盘操作来获取一个图片：一个读取目录元数据到内存、第二个装载inode到内存、最后读取文件内容。 为了继续减少磁盘操作，我们让图片存储服务器明确的缓存NAS设备返回的文件“句柄”。第一次读取一个文件时，图片存储服务器正常打开一个文件，将文件名与文件“句柄”的映射缓存到memcache中。同时，我们在os内核中添加了一个通过句柄打开文件的接口，当查询被缓存的文件时，图片存储服务器直接用此接口和“句柄”参数打开文件。遗憾的是，文件“句柄”缓存改进不大，因为越冷门的图片越难被缓存到（没有解决long tail问题）。值得讨论的是可以将所有文件“句柄”缓存到memcache，不过这也需要NAS设备能缓存所有的inode信息，这么做是非常昂贵的。总结一下，我们从NAS方案吸取的主要教训是，仅针对缓存——不管是NAS设备缓存还是额外的像memcache缓存——对减少磁盘操作的改进是有限的。存储系统终究是要处理long tail请求（不热门图片）。 讨论我们很难提出一个指导方针关于何时应该构建一个自定义的存储系统。下面是我们在最终决定搭建Haystack之前的一些思考，希望能给大家提供参考。 面对基于NFS设计的瓶颈，我们探讨了是否可以构建一个类似GFS的系统。而我们大部分用户数据都存储在Mysql数据库，文件存储主要用于开发工作、日志数据以及图片。NAS设备其实为这些场景提供了性价比很好的方案。此外，我们补充了hadoop以供海量日志数据处理。面对图片服务的long tail问题，Mysql、NAS、Hadoop都不太合适。 我们面临的困境可简称为“已存在存储系统缺乏合适的RAM-to-disk比率”。然而，没有什么比率是绝对正确的。系统需要足够的内存才能缓存所有的文件系统元数据。在我们基于NAS的方案中，一个图片对应到一个文件，每个文件需要至少一个inode，这已经占了几百byte。提供足够的内存太昂贵。所以我们决定构建一个定制存储系统，减少每个图片的元数据总量，以便能有足够的内存。相对购买更多的NAS设备，这是更加可行的、性价比更好的方案。 设计和实现Facebook使用CDN来支撑热门图片查询，结合Haystack则解决了它的long tail问题。如果web站点在查询静态内容时遇到I/O瓶颈，传统方案就是使用CDN，它为下游的存储系统挡住了绝大部分的查询请求。在Facebook，为了传统的、廉价的的底层存储不受I/O摆布，CDN往往需要缓存难以置信的海量静态内容。 上面已经论述过，在不久的将来，CDN也不能完全的解决我们的问题，所以我们设计了Haystack来解决这个严重瓶颈：磁盘操作。我们接受long tail请求必然导致磁盘操作的现实，但是会尽量减少除了访问真实图片数据之外的其他操作。Haystack有效的减少了文件系统元数据的空间，并在内存中保存所有元数据。 每个图片存储为一个文件将会导致元数据太多，难以被全部缓存。Haystack的对策是：将多个图片存储在单个文件中，控制文件个数，维护大型文件，我们将论述此方案是非常有效的。另外，我们强调了它设计的简洁性，以促进快速的实现和部署。我们将以此核心技术展开，结合它周边的所有架构组件，描述Haystack是如何实现了一个高可靠、高可用的存储系统。在下面对Haystack的介绍中，需要区分两种元数据，不要混淆。一种是应用元数据，它是用来为浏览器构造检索图片所需的URL；另一种是文件系统元数据，用于在磁盘上检索文件。 概览Haystack架构包含3个核心组件：Haytack Store、Haystack Directory和Haystack Cache（简单起见我们下面就不带Haystack前缀了）。Store是持久化存储系统，并负责管理图片的文件系统元数据。Store将数据存储在物理的卷上。比如，在一台机器上提供100个物理卷，每个提供100GB的存储容量，整台机器则可以支撑10TB的存储。更进一步，不同机器上的多个物理卷将对应一个逻辑卷。Haystack将一个图片存储到一个逻辑卷时，图片被写入到所有对应的物理卷。这个冗余可避免由于硬盘故障，磁盘控制器bug等导致的数据丢失。Directory维护了逻辑到物理卷的映射以及其他应用元数据，比如某个图片寄存在哪个逻辑卷、某个逻辑卷的空闲空间等。Cache的功能类似我们系统内部的CDN，它帮Store挡住热门图片的请求（可以缓存的就绝不交给下游的持久化存储）。在独立设计Haystack时，我们要设想它处于一个没有CDN的大环境中，即使有CDN也要预防其节点故障导致大量请求直接进入存储系统，所以Cache是十分必要的。 图3说明了Store、Directory、Cache是如何协作的，以及如何与外部的浏览器、web服务器、CDN和存储系统交互。在Haystack架构中，浏览器会被引导至CDN或者Cache上。需要注意的是Cache本质上也是一个CDN，为了避免困惑，我们使用“CDN”表示外部的系统、使用“Cache”表示我们内部的系统。有一个内部的缓存设施能减少对外部CDN的依赖。 当用户访问一个页面，web服务器使用Directory为每个图片来构建一个URL（Directory中有足够的应用元数据来构造URL）。URL包含几块信息，每一块内容可以对应到从浏览器访问CDN(或者Cache)直至最终在一台Store机器上检索到图片的各个步骤。一个典型的URL如下：1http://&lt;CDN&gt;/&lt;Cache&gt;/&lt;Machine id&gt;/&lt;Logical volume, Photo&gt; 第一个部分指明了从哪个CDN查询此图片。到CDN后它使用最后部分的URL（逻辑卷和图片ID）即可查找缓存的图片。如果CDN未命中缓存，它从URL中删除相关信息，然后访问Cache。Cache的查找过程与之类似，如果还没命中，则去掉相关信息，请求被发至指定的Store机器()。如果请求不经过CDN直接发至Cache，其过程与上述类似，只是少了CDN这个环节。 上图说明了在Haystack中的上传流程。用户上传一个图片时，她首先发送数据到web服务器。web服务器随后从Directory中请求一个可写逻辑卷。最后，web服务器为图片分配一个唯一的ID，然后将其上传至逻辑卷对应的每个物理卷。 ### Haystack Directory Directory提供4个主要功能。首先，它提供一个从逻辑卷到物理卷的映射。web服务器上传图片和构建图片URL时都需要使用这个映射。第二，Directory在分配写请求到逻辑卷、分配读请求到物理卷时需保证负载均衡。第三，Directory决定一个图片请求应该被发至CDN还是Cache，这个功能可以让我们动态调整是否依赖CDN。第四，Directory指明那些逻辑卷是只读的（只读限制可能是源自运维原因、或者达到存储容量上限；为了运维方便，我们以机器粒度来标记卷的只读）。 当我们增加新机器以增大Store的容量时，那些新机器是可写的；仅仅可写的机器会收到upload请求。随时间流逝这些机器的可用容量会不断减少。当一个机器达到容量上限，我们标记它为只读，在下一个子章节我们将讨论如何这个特性如何影响Cache和Store。 Directory将应用元数据存储在一个冗余复制的数据库，通过一个PHP接口访问，也可以换成memcache以减少延迟。当一个Store机器故障、数据丢失时，Directory在应用元数据中删除对应的项，新Store机器上线后则接替此项。 3.2章节是整篇文章中唯一一处译者认为没有解释清楚的环节。结合3.1章节中的URL结构解析部分，读者可以发现Directory需要拿到图片的“原始URL”（页面html中link的URL），再结合应用元数据，就可以构造出“引导URL”以供下游使用。从3.2中我们知道Directory必然保存了逻辑卷到物理卷的映射，仅用此映射+原始URL足够发掘其他应用元数据吗？原始URL中到底包含了什么信息（论文中没看到介绍）？我们可以做个假设，假如原始URL中仅仅包含图片ID，那Directory如何得知它对应哪个逻辑卷（必须先完成这一步映射，才能继续挖掘更多应用元数据）？Directory是否在upload阶段将图片ID与逻辑卷的映射也保存了？如果是，那这个映射的数据量不能忽略不计，论文也不该一笔带过。 从原文一些细枝末节的描述中，译者认为Directory确实保存了很多与图片ID相关的元数据（存储在哪个逻辑卷、cookie等）。但整篇论文译者也没找到对策，总感觉这样性价比太低，不符合Haystack的作风。对于这个疑惑，文章末尾扩展阅读部分将尝试解答。读者先认为其具备此能力吧。 Haystack CacheCache会从CDN或者直接从用户浏览器接收到图片查询请求。Cache的实现可理解为一个分布式Hash Table，使用图片ID作为key来定位缓存的数据。如果Cache未命中，Cache则根据URL从指定Store机器上获取图片，视情况回复给CDN或者用户浏览器。 我们现在强调一下Cache的一个重要行为概念。只有当符合两种条件之一时它才会缓存图片：(a)请求直接来自用户浏览器而不是CDN；(b)图片获取自一个可写的Store机器。第一个条件的理由是一个请求如果在CDN中没命中（非热门图片），那在我们内部缓存也不太需要命中（即使此图片开始逐渐活跃，那也能在CDN中命中缓存，这里无需多此一举；直接的浏览器请求说明是不经过CDN的，那就需要Cache代为CDN，为其缓存）。第二个条件的理由是间接的，有点经验论，主要是为了保护可写Store机器；原因挺有意思，大部分图片在上传之后很快会被频繁访问（比如某个美女新上传了一张自拍），而且文件系统在只有读或者只有写的情况下执行的更好，不太喜欢同时并发读写。如果没有Cache，可写Store机器往往会遭遇频繁的读请求。因此，我们甚至会主动的推送最近上传的图片到Cache。 Haystack StoreStore机器的接口设计的很简约。读操作只需提供一些很明确的元数据信息，包括图片ID、哪个逻辑卷、哪台物理Store机器等。机器如果找到图片则返回其真实数据，否则返回错误信息。 每个Store机器管理多个物理卷。每个物理卷存有百万张图片。读者可以将一个物理卷想象为一个非常大的文件（100GB），保存为’/hay/haystack&lt;logical volume id&gt;’。Store机器仅需要逻辑卷ID和文件offset就能非常快的访问一个图片。这是Haystack设计的主旨：不需要磁盘操作就可以检索文件名、偏移量、文件大小等元数据。Store机器会将其下所有物理卷的文件描述符（open的文件“句柄”，卷的数量不多，数据量不大）缓存在内存中。同时，图片ID到文件系统元数据（文件、偏移量、大小等）的映射（后文简称为“内存中映射”）是检索图片的重要条件，也会全部缓存在内存中。 现在我们描述一下物理卷和内存中映射的结构。一个物理卷可以理解为一个大型文件，其中包含一系列的needle。每个needle就是一张图片。图5说明了卷文件和每个needle的格式。Table1描述了needle中的字段。 为了快速的检索needle，Store机器需要为每个卷维护一个内存中的key-value映射。映射的Key就是（needle.key+needle.alternate_key）的组合，映射的Value就是needle的flag、size、卷offset（都以byte为单位）。如果Store机器崩溃、重启，它可以直接分析卷文件来重新构建这个映射（构建完成之前不处理请求）。下面我们介绍Store机器如何响应读写和删除请求（Store仅支持这些操作）。 译者注: 从Table1我们看到needle.key就是图片ID，为何仅用图片ID做内存中映射的Key还不够，还需要一个alternate_key？这是因为一张照片会有4份副本，它们的图片ID相同，只是类型不同（比如大图、小图、缩略图等），于是将图片ID作为needle.key，将类型作为needle.alternate_key。根据译者的理解，内存中映射不是一个简单的HashMap结构，而是类似一个两层的嵌套HashMap，Map&lt;long/*needle.key*/,Map&lt;int/*alternate_key*/,Object&gt;&gt;。这样做可以让4个副本共用同一个needle.key，避免为重复的内容浪费内存空间。 读取图片Cache机器向Store机器请求一个图片时，它需要提供逻辑卷id、key、alternate key，和cookie。cookie是个数字，嵌在URL中。当一张新图片被上传，Directory为其随机分配一个cookie值，并作为应用元数据之一存储在Directory。它就相当于一张图片的“私人密码”，此密码可以保证所有发往Cache或CDN的请求都是经过Directory“批准”的（Cache和Store都持有图片的cookie，若用户自己在浏览器中伪造、猜测URL或发起攻击，则会因为cookie不匹配而失败，从而保证Cache、Store能放心处理合法的图片请求）。 当Store机器接收到Cache机器发来的图片查询请求，它会利用内存中映射快速的查找相关的元数据。如果图片没有被删除，Store则在卷文件中seek到相应的offset，从磁盘上读取整个needle（needle的size可以提前计算出来），然后检查cookie和数据完整性，若全部合法则将图片数据返回到Cache机器。 写入图片上传一个图片到Haystack时，web服务器向Directory咨询得到一个可写逻辑卷及其对应的多台Store机器，随后直接访问这些Store机器，向其提供逻辑卷id、key、alternate key、cookie和真实数据。每个Store机器为图片创建一个新needle，append到相应的物理卷文件，更新内存中映射。过程很简单，但是append-only策略不能很好的支持修改性的操作，比如旋转（图片顺时针旋转90度之类的）。Haystack并不允许覆盖needle，所以图片的修改只能通过添加一个新needle，其拥有相同的key和alternate key。如果新needle被写入到与老needle不同的逻辑卷，则只需要Directory更新它的应用元数据，未来的请求都路由到新逻辑卷，不会获取老版本的数据。如果新needle写入到相同的逻辑卷，Store机器也只是将其append到相同的物理卷中。Haystack利用一个十分简单的手段来区分重复的needle，那就是判断它们的offset（新版本的needle肯定是offset最高的那个），在构造或更新内存中映射时如果遇到相同的needle，则用offset高的覆盖低的。 图片删除在删除图片时，Store机器将内存中映射和卷文件中相应的flag同步的设置为已删除（软删除机制，此刻不会删除needle的磁盘数据）。当接收到已删除图片的查询请求，Store会检查内存中flag并返回错误信息。值得注意的是，已删除needle依然占用的空间是个问题，我们稍后将讨论如何通过压缩技术来回收已删除needle的空间。 索引文件Store机器使用一个重要的优化——索引文件——来帮助重启初始化。尽管理论上一个机器能通过读取所有的物理卷来重新构建它的内存中映射，但大量数据（TB级别）需要从磁盘读取，非常耗时。索引文件允许Store机器快速的构建内存中映射，减少重启时间。 Store机器为每个卷维护一个索引文件。索引文件可以想象为内存中映射的一个“存档”。索引文件的布局和卷文件类似，一个超级块包含了一系列索引记录，每个记录对应到各个needle。索引文件中的记录与卷文件中对应的needle必须保证相同的存储顺序。图6描述了索引文件的布局，Table2解释了记录中的不同的字段。 使用索引帮助重启稍微增加了系统复杂度，因为索引文件都是异步更新的，这意味着当前索引文件中的“存档”可能不是最新的。当我们写入一个新图片时，Store机器同步append一个needle到卷文件末尾，并异步append一个记录到索引文件。当我们删除图片时，Store机器在对应needle上同步设置flag，而不会更新索引文件。这些设计决策是为了让写和删除操作更快返回，避免附加的同步磁盘写。但是也导致了两方面的影响：一个needle可能没有对应的索引记录、索引记录中无法得知图片已删除。 我们将对应不到任何索引记录的needle称为“孤儿”。在重启时，Store机器顺序的检查每个孤儿，重新创建匹配的索引记录，append到索引文件。我们能快速的识别孤儿是因为索引文件中最后的记录能对应到卷文件中最后的非孤儿needle。处理完孤儿问题，Store机器则开始使用索引文件初始化它的内存中映射。 由于索引记录中无法得知图片已删除，Store机器可能去检索一个实际上已经被删除的图片。为了解决这个问题，可以在Store机器读取整个needle后检查其flag，若标记为已删除，则更新内存中映射的flag，并回复Cache此对象未找到。 文件系统Haystack可以理解为基于通用的类Unix文件系统搭建的对象存储，但是某些特殊文件系统能更好的适应Haystack。比如，Store机器的文件系统应该不需要太多内存就能够在一个大型文件上快速的执行随机seek。当前我们所有的Store机器都在使用的文件系统是XFS，一个基于“范围(extent)”的文件系统。XFS有两个优势：首先，XFS中邻近的大型文件的”blockmap”很小，可放心使用内存存储；第二，XFS提供高效文件预分配，减轻磁盘碎片等问题。 使用XFS，Haystack可以在读取一张图片时完全避免检索文件系统元数据导致的磁盘操作。但是这并不意味着Haystack能保证读取单张图片绝对只需要一个磁盘操作。在一些极端情况下会发生额外的磁盘操作，比如当图片数据跨越XFS的“范围(extent)”或者RAID边界时。不过Haystack会预分配1GB的“范围(extent)”、设置RAID stripe大小为256KB，所以实际上我们很少遭遇这些极端场景。 故障恢复对于运行在普通硬件上的大规模系统，容忍各种类型的故障是必须的，包括硬盘驱动故障、RAID控制器错误、主板错误等，Haystack也不例外。我们的对策由两个部分组成——一个为侦测、一个为修复。 为了主动找到有问题的Store机器，我们维护了一个后台任务，称之为pitchfork，它周期性的检查每个Store机器的健康度。pitchfork远程的测试到每台Store机器的连接，检查其每个卷文件的可用性，并尝试读取数据。如果pitchfork确定某台Store机器没通过这些健康检查，它会自动标记此台机器涉及的所有逻辑卷为只读。我们的工程师将在线下人工的检查根本故障原因。 一旦确诊，我们就能快速的解决问题。不过在少数情况下，需要执行一个更加严厉的bulk同步操作，此操作需要使用复制品中的卷文件重置某个Store机器的所有数据。Bulk同步发生的几率很小（每个月几次），而且过程比较简单，只是执行很慢。主要的瓶颈在于bulk同步的数据量经常会远远超过单台Store机器NIC速度，导致好几个小时才能恢复。我们正积极解决这个问题。 优化Haystack的成功还归功于几个非常重要的细节优化。 压缩压缩操作是直接在线执行的，它能回收已删除的、重复的needle所占据的空间。Store机器压缩卷文件的方式是，逐个复制needle到一个新的卷文件，并跳过任何重复项、已删除项。在压缩时如果接收到删除操作，两个卷文件都需处理。一旦复制过程执行到卷文件末尾，所有对此卷的修改操作将被阻塞，新卷文件和新内存中映射将对前任执行原子替换，随后恢复正常工作。 节省更多内存上面描述过，Store机器会在内存中映射中维护一个flag，但是目前它只会用来标记一个needle是否已删除，有点浪费。所以我们通过设置偏移量为0来表示图片已删除，物理上消除了这个flag。另外，映射Value中不包含cookie，当needle从磁盘读出之后Store才会进行cookie检查。通过这两个技术减少了20%的内存占用。 当前，Haystack平均为每个图片使用10byte的内存。每个上传的图片对应4张副本，它们共用同一个key（占64bits），alternate keys不同（占32bits），size不同（占16bits），目前占用(64+(32+16)*4)/8=32个bytes。另外，对于每个副本，Haystack在用hash table等结构时需要消耗额外的2个bytes，最终总量为一张图片的4份副本共占用40bytes。作为对比，一个xfs_inode_t结构在Linux中需占用536bytes。 批量上传磁盘在执行大型的、连续的写时性能要优于大量小型的随机写，所以我们尽量将相关写操作捆绑批量执行。幸运的是，很多用户都会上传整个相册到Facebook，而不是频繁上传单个图片。因此只需做一些巧妙的安排就可以捆绑批量upload，实现大型、连续的写操作。 章节4、5、6是实验和总结等内容，这里不再赘述了。 扩展阅读提到CDN和分布式文件存储就不得不提到淘宝，它的商品图片不会少于Facebook的个人照片。其著名的CDN+TFS的解决方案由于为公司节省了巨额的预算开支而获得创新大奖，团队成员也得到不菲的奖金（羡慕嫉妒恨）。淘宝的CDN技术做了非常多的技术创新和突破，不过并非本文范畴，接下来的讨论主要是针对Haystack与TFS在存储、检索环节的对比，并尝试提取出此类场景常见的技术难点。（译者对TFS的理解仅限于介绍文档，若有错误望读者矫正） 淘宝CDN、TFS的介绍请移步 http://www.infoq.com/cn/presentations/zws-taobao-image-store-cdnhttp://tfs.taobao.org/index.html 注意：下文中很多术语（比如应用元数据、Store、文件系统元数据等，都是基于本篇论文的上下文，请勿混淆） 上图是整个CDN+TFS解决方案的全貌，对应本文就是图3。CDN在前三层上实现了各种创新和技术突破，不过并非本文焦点，这里主要针对第四层Storage（淘宝的分布式文件系统TFS），对比Haystack，看其是否也解决了long tail问题。下面是TFS的架构概览： 从粗粒度的宏观视角来看，TFS与Haystack的最大区别就是： TFS只care存储层面，它没有Haystack Cache组件；Haystack期望提供的是从浏览器、到CDN、到最终存储的一整套解决方案，架构定位稍有不同，Haystack也是专门为这种场景下的图片服务所定制的，做了很多精细的优化；TFS的目标是通用分布式文件存储，除了CDN还会支持其他各种场景。 到底是定制一整套优化的解决方案，还是使用通用分布式文件存储平台强强联手？Facebook的工程师也曾纠结过（章节2.3），这个没有标准答案，各有所长，视情况去选择最合适的方案吧。 下面我们以本文中关注的一些难点来对比一下双方的实现： 存储机器上的文件结构、文件系统元数据对策Haystack的机器上维护了少量的大型物理卷文件，其中包含一系列needle来存储小文件，同时needle的文件系统元数据被全量缓存、持久化“存档”。 在TFS中（后文为清晰起见，引用TFS文献的内容都用淘宝最爱的橙色展示）： “……在TFS中，将大量的小文件(实际用户文件)合并成为一个大文件，这个大文件称为块(Block)。TFS以Block的方式组织文件的存储……” “……!DataServer进程会给Block中的每个文件分配一个ID(File ID，该ID在每个Block中唯一)，并将每个文件在Block中的信息存放在和Block对应的Index文件中。这个Index文件一般都会全部load在内存……” 看来面对可怜的操作系统，大家都不忍心把海量的小文件直接放到它的文件系统上，合并成super block，维护super block中各entry的元数据和索引信息（并全量缓存），才是王道。这里TFS的Block应该对应到Haystack中的一个物理卷。 分布式协调调度、应用元数据策略Haystack在接收到读写请求时，依靠Directory分析应用元数据，再结合一定策略（如负载均衡、容量、运维、只读、可写等），决定请求被发送到哪台Store机器，并向Store提供足够的存储或检索信息。Directory负责了整体分布式环境的协调调度、应用元数据管理职能，并基于此帮助实现了系统的可扩展性、容错性。 在TFS中： “……!NameServer主要功能是: 管理维护Block和!DataServer相关信息,包括!DataServer加入，退出, 心跳信息, block和!DataServer的对应关系建立，解除。正常情况下，一个块会在!DataServer上存在， 主!NameServer负责Block的创建，删除，复制，均衡，整理……” “……每一个Block在整个集群内拥有唯一的编号，这个编号是由NameServer进行分配的，而DataServer上实际存储了该Block。在!NameServer节点中存储了所有的Block的信息……” TFS中与Directory对应的就是NameServer了，职责大同小异，就是分布式协调调度和应用元数据分配管理，并基于此实现系统的平滑扩容、故障容忍。下面专门讨论一下这两个重要特性。 扩展性Haystack和TFS都基于（分布式协调调度+元数据分配管理）实现了非常优雅的可扩展方案。我们先回顾一下传统扩展性方案中的那些简单粗暴的方法。 最简单最粗暴的场景： 现在有海量的数据，比如data [key : value]，有100台机器，通过一种策略让这些数据能负载均衡的发给各台机器。策略可以是这样，int index=Math.abs(key.hashCode)%100，这就得到了一个唯一的、确定的、[0,99]的序号，按此序号发给对应的某台机器，最终能达到负载均衡的效果。此方案的粗暴显而易见，当我们新增机器后（比如100变成130），大部分老数据的key执行此策略后得到的index会发生变化，这也就意味着对它们的检索都会发往错误的机器，找不到数据。 稍微改进的场景是： 现在有海量的数据，比如data [key : value]，我假想自己是高富帅，有一万台机器，同样按照上述的策略进行路由。但是我只有100台机器，这一万台是假想的，怎么办？先给它们一个称号，叫虚拟节点（简称vnode，vnode的序号简称为vnodeId），然后想办法将vnode与真实机器建立多对一映射关系（每个真实机器上100个vnode），这个办法可以是某种策略，比如故技重施对vnodeId%100得到[0,99]的机器序号，或者在数据库中建几张表维护一下这个多对一的映射关系。在路由时，先按老办法得到vnodeId，再执行一次映射，找到真实机器。这个方案还需要一个架构假设：我的系统规模在5年内都不需要上涨到一万台机器（5年差不多了，像我等码农估计一辈子也玩不了一万台机器的集群吧），因此10000这个数字“永远”不会变，这就保证了一个key永远对应某个vnodeId，不会发生改变。然后在扩容时，我们改变的是vnode与真实机器的映射关系，但是此映射关系一改，也会不可避免的导致数据命中失败，因为必然会产生这样的现象：某个vnodeId（v1）原先是对应机器A的，现在变成了机器B。但是相比之前的方案，现在已经好很多了，我们可以通过运维手段先阻塞住对v1的读写请求，然后执行数据迁移（以已知的vnode为粒度，而不是千千万万个未知的data，这种迁移操作还是可以接受的），迁移完毕后新机器开始接收请求。做的更好一点，可以不阻塞请求，想办法做点容错处理和写同步之类的，可以在线无痛的完成迁移。 上面两个老方案还可以加上一致性Hash等策略来尽量避免数据命中失败和数据迁移。但是始终逃避不了这样一个公式：1int machine_id=function(data.key , x) machine_id指最终路由到哪台机器，function代表我们的路由策略函数，data.key就是数据的key（数据ID之类的），x在第一个方案里就是机器数量100，在第二个方案里就是vnode数量+(vnode与机器的映射关系)。在这个公式里，永远存在了x这个未知数，一旦它风吹草动，function的执行结果就可能改变，所以它逃避不了命中失败。 只有当公式变成下面这个，才能绝对避免： 12Map&lt;data.key,final machine_id&gt; map = xxx; int machine_id=map.get(data.key); 注意map只是个理论上的结构，这里只是简单的伪代码，并不强制它是个简单的&lt;key-value&gt;结构，它的结构可能会更复杂，但是无论怎么复杂，此map都真实的、明确的存在，其效果都是——用data.key就能映射到machine_id，找到目标机器，不管是直接，还是间接，反正不是用一个function去动态计算得到。map里的final不符合语法，加在这里是想强调，此map一旦为某个data.key设置了machine_id，就永不改变（起码不会因为日常扩容而改变）。当增加机器时，此map的已有值也不会受到影响。这样一个没有未知数x的公式，才能保证新老数据来了都能根据key拿到一个永远不变的machine_id，永远命中成功。 因此我们得出这样一个结论，只要拥有这样一个map，系统就能拥有非常优雅平滑的可扩展潜力。当系统扩容时，老的数据不会命中失败，在分布式协调调度的保证下，新的增量数据会更倾向于写入新机器，整个集群的负载会逐渐均衡。 很显然Haystack和TFS都做到了，下面忽略其他细节问题，着重讨论一下它们是如何装备上这个map的。 读者回顾一下3.2章节留下的那个疑惑——原始URL中到底包含什么信息，是不是只有图片ID？Directory到底需不需要维护图片ID到逻辑卷的映射？ 这个“图片ID到逻辑卷的映射”，就是我们需要的map，用图片ID（data.key）能get到逻辑卷ID（此值是upload时就明确分配的，不会改变），再间接从“逻辑卷到物理卷映射”中就能get到目标Store机器；无论是新增逻辑卷还是新增物理卷，“图片ID到逻辑卷的映射”中的已有值都可以不受影响。这些都符合map的行为定义。 Haystack也因此，具备了十分优雅平滑的可扩展能力。但是译者提到的疑惑并没有解答——“这个映射（图片ID到逻辑卷的映射）的数据量不能忽略不计，论文也不该一笔带过” 作者提到过memcache，也许这就是相关的解决方案，此数据虽然不小，但是也没大到望而生畏的地步。不过我们依然可以发散一下，假如Haystack没保存这个映射呢？ 这就意味着原始URL不只包含图片ID，还包含逻辑卷ID等必要信息。这样也是遵循map的行为定义的，即使map的信息没有集中存储在系统内，但是却分散在各个原始URL中，依然存在。不可避免的，这些信息就要在upload阶段返回给业务系统（比如Facebook的照片分享应用系统），业务系统需要理解、存储和处理它们（随后再利用它们组装为原始URL去查询图片）。这样相当于把map的维护工作分担给了各个用户系统，这也是让人十分痛苦的，导致了不可接受的耦合。 我们可以看看TFS的解决方案： “……TFS的文件名由块号和文件号通过某种对应关系组成，最大长度为18字节。文件名固定以T开始，第二字节为该集群的编号(可以在配置项中指定，取值范围 1~9)。余下的字节由Block ID和File ID通过一定的编码方式得到。文件名由客户端程序进行编码和解码，它映射方式如下图……” “……根据TFS文件名解析出Block ID和block中的File ID.……dataserver会根据本地记录的信息来得到File ID所在block的偏移量，从而读取到正确的文件内容……” 一切，迎刃而解了…… 这个方案可以称之为“结构化ID”、“聚合ID”，或者是“命名规则大于配置”。当我们纠结于仅仅有图片ID不够时，可以给ID简单的动动手脚，比如ID是long类型，8个byte，左边给点byte用于存储逻辑卷ID，剩下的用于存储真实的图片ID（某些场景下还可以多截几段给更多的元数据），于是既避免了保存大量的映射数据，又避免了增加系统间的耦合，鱼和熊掌兼得。不过这个方案对图片ID有所约束，也不支持自定义的图片名称，针对这个问题，TFS在新版本中： “……metaserver是我们在2.0版本引进的一个服务. 用来存储一些元数据信息, 这样原本不支持自定义文件名的 TFS 就可以在 metaserver 的帮助下, 支持自定义文件名了.……” 此metaserver的作用无疑就和Directory中部分应用元数据相关的职责类似了。个人认为可以两者结合双管齐下，毕竟自定义文件名这种需求应该不是主流。 值得商榷的是，全量保存这部分应用元数据其实还是有很多好处的，最典型的就是顺带保存的cookie，有效的帮助Haystack不受伪造URL攻击的困扰，这个问题不知道TFS是如何解决的（大量的文件检索异常势必会影响系统性能）。如果Haystack的作者能和TFS的同学们做个交流，说不定大家都能少走点弯路吧（这都是后话了~） 小结一下，针对第三个可扩展性痛点，译者描述了传统方案的缺陷，以及Haystack和TFS是如何弥补了这些缺陷，实现了平滑优雅的可扩展能力。此小节的最后再补充一个TFS的特性： “……同时，在集群负载比较轻的时候，!NameServer会对!DataServer上的Block进行均衡，使所有!DataServer的容量尽早达到均衡。进行均衡计划时，首先计算每台机器应拥有的blocks平均数量，然后将机器划分为两堆，一堆是超过平均数量的，作为移动源；一类是低于平均数量的，作为移动目的……” 均衡计划的职责是在负载较低的时候（深夜），按计划执行Block数据的迁移，促进整体负载更加均衡。根据译者的理解，此计划会改变公式中的map，因为根据文件名拿到的BlockId对应的机器可能发生变化，这也是它为何要在深夜负载较低时按计划缜密执行的原因。其效果是避免了因为运维操作等原因导致的数据分布不均。 容错性Haystack的容错是依靠：一个逻辑卷对应多个物理卷（不同机器上）；“客户端”向一个逻辑卷的写操作会翻译为对多个物理卷的写，达到冗余备份；机器故障时Directory优雅的修改应用元数据（在牵涉到的逻辑卷映射中删除此机器的物理卷项）、或者标记只读，继而指导路由过程（分布式协调调度）将请求发送到后备的节点，避免请求错误；通过bulk复制重置来安全的恢复数据。等等。 在TFS中： “……TFS可以配置主辅集群，一般主辅集群会存放在两个不同的机房。主集群提供所有功能，辅集群只提供读。主集群会把所有操作重放到辅集群。这样既提供了负载均衡，又可以在主集群机房出现异常的情况不会中断服务或者丢失数据。……” “……每一个Block会在TFS中存在多份，一般为3份，并且分布在不同网段的不同!DataServer上……” “……客户端向master dataserver开始数据写入操作。master server将数据传输为其他的dataserver节点，只有当所有dataserver节点写入均成功时，master server才会向nameserver和客户端返回操作成功的信息。……” 可以看出冗余备份+协调调度是解决这类问题的惯用范式，在大概思路上两者差不多，但是有几个技术方案却差别很大： 第一，冗余写机制。Haystack Store是将冗余写的责任交给“客户端”（发起写操作的客户端，就是图3中的web server），“客户端”需要发起多次写操作到不同的Store机器上；而TFS是依靠自身的master-slave机制，由master向slave复制。 第二，机房容错机制。TFS依然是遵循master-slave机制，集群也分主辅，主辅集群分布在不同机房，主集群负责重放数据操作到辅集群。而Haystack在这方面没有详细介绍，只是略微提到“……Haystack复制每张图片到地理隔离的多个地点……” 针对上面两点，按译者的理解，Haystack可能更偏向于对等结构的设计，也就是说没有master、slave之分，各个Store是对等的节点，没有谁负责给谁复制数据，“客户端”向各个Store写入数据，一视同仁。 不考虑webserver、Directory等角色，只考虑Store，来分析一下它的容错机制：如果单台Store挂了，Directory在应用元数据的相关逻辑卷映射中删除此台机器的物理卷（此过程简称为“调整逻辑物理映射”），其他“对等”的物理卷能继续服务，没有问题；一整个机房挂了，Directory处理过程和单台故障相同，只是会对此机房中每台机器都执行一遍“调整逻辑物理映射”，由于逻辑卷到物理卷的映射是在Directory中明确维护的，所以只要在维护和管理过程中确保一个逻辑卷下不同的物理卷分布在不同的机房，哪怕在映射中删除一整个机房所有机器对应的物理卷，各个逻辑卷下依然持有到其他机房可用物理卷的映射，依然有对等Store的物理卷做后备，没有问题。 主从结构和对等结构各有所长，视情况选择。对等结构看似简洁美好，也有很多细节上的妥协；主从结构增加了复杂度，比如严格角色分配、约定角色行为等等（TFS的辅集群为何只读？在主集群挂掉时是否依然只读？这些比较棘手也是因为此复杂度吧） 第三，修复机制。Haystack的修复机制依靠周期性后台任务pitchfork和离线bulk重置等。在TFS中： “……Dataserver后台线程介绍……” “……心跳线程……这里的心跳是指Ds向Ns发的周期性统计信息……负责keepalive……汇报block的工作……” “……检查线程……修复check_filequeue中的逻辑块……每次对文件进行读写删操作失败的时候，会try_add_repair_task(blockid, ret)来将ret错误的block加入check_filequeue中……若出错则会请求Ns进行update_block_info……” 除了类似的远程心跳机制，TFS还多了在DataServer上对自身的错误统计和自行恢复，必要时还会请求上级（NameServer）帮助恢复。 文件系统 Haystack提到了预分配、磁盘碎片、XFS等方案，TFS中也有所涉及： “……在!DataServer节点上，在挂载目录上会有很多物理块，物理块以文件的形式存在磁盘上，并在!DataServer部署前预先分配，以保证后续的访问速度和减少碎片产生。为了满足这个特性，!DataServer现一般在EXT4文件系统上运行。物理块分为主块和扩展块，一般主块的大小会远大于扩展块，使用扩展块是为了满足文件更新操作时文件大小的变化。每个Block在文件系统上以“主块+扩展块”的方式存储。每一个Block可能对应于多个物理块，其中包括一个主块，多个扩展块。在DataServer端，每个Block可能会有多个实际的物理文件组成：一个主Physical Block文件，N个扩展Physical Block文件和一个与该Block对应的索引文件……” 各有各的考究吧，比较了解底层的读者可以深入研究下。 删除和压缩Haystack使用软删除（设置flag）、压缩回收来支持delete操作，在TFS中： “……压缩线程（compact_block.cpp）……真正的压缩线程也从压缩队列中取出并进行执行（按文件进行，小文件合成一起发送）。压缩的过程其实和复制有点像，只是说不需要将删除的文件数据以及index数据复制到新创建的压缩块中。要判断某个文件是否被删除，还需要拿index文件的offset去fileinfo里面取删除标记，如果标记不是删除的，那么就可以进行write_raw_data的操作，否则则滤过……” 可见两者大同小异，这也是此类场景中常用的解决机制。 总结本篇论文以long tail无法避免出发，探究了文件元数据导致的I/O瓶颈，推导了海量小文件的存储和检索方案，以及如何与CDN等外部系统配合搭建出整套海量图片服务。其在各个痛点的解决方案以及简约而不简单的设计值得我们学习。文章末尾将这些痛点列出并与淘宝的解决方案逐一对比，以供读者发散。]]></content>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POI两种读取Excel的方式]]></title>
    <url>%2F2017%2F12%2F18%2Fpoi-read-excel%2F</url>
    <content type="text"><![CDATA[前言在实际项目上遇到了上传 Excel 并解析到数据库中的需求，经分析后选择 Apache POI 来实现，故事也就开始了…… 常规模式123456789101112131415161718192021222324252627282930313233343536373839File file = new File(yourFilepath);FileInputStream is = new FileInputStream(file);Workbook workbook = WorkbookFactory.create(is);FormulaEvaluator formulaEvaluator = workbook.getCreationHelper().createFormulaEvaluator();for (Sheet sheet : workbook) &#123; for (Row row : sheet) &#123; for (Cell cell : row) &#123; // Alternatively, get the value and format it yourself switch (formulaEvaluator.evaluateInCell(cell).getCellTypeEnum()) &#123; case STRING: System.out.println(cell.getRichStringCellValue().getString()); break; case NUMERIC: if (DateUtil.isCellDateFormatted(cell)) &#123; System.out.println(cell.getDateCellValue()); &#125; else &#123; System.out.println(cell.getNumericCellValue()); &#125; break; case BOOLEAN: System.out.println(cell.getBooleanCellValue()); break; case FORMULA: System.out.println(cell.getCellFormula()); break; case BLANK: System.out.println(); break; default: System.out.println(); &#125; &#125; &#125;&#125; 优点：取值方便，可供调用方法多； 缺点：一次性读完值，在读取大量数据时可能会出现堆溢出。 事件驱动模式优点：逐个单元格读取，理论上多大数据都能处理； 缺点：调用方法较少，需要自己动手堆数据进行处理。 查看 Excel XML 映射关系——新增 Excel 后缀名 .zip，打开 xl &gt; worksheets &gt; sheet1.xml ，查看对应的标签节点信息，通过 POI 提供的方法来读取。 Excel XML 片段截取： 1234567891011121314151617181920212223242526&lt;row r="1" spans="1:8" s="4" customFormat="1" ht="18"&gt; &lt;c r="A1" s="3" t="s"&gt; &lt;v&gt;6&lt;/v&gt; &lt;/c&gt; &lt;c r="B1" s="3" t="s"&gt; &lt;v&gt;7&lt;/v&gt; &lt;/c&gt; &lt;c r="C1" s="3" t="s"&gt; &lt;v&gt;8&lt;/v&gt; &lt;/c&gt; &lt;c r="D1" s="3" t="s"&gt; &lt;v&gt;9&lt;/v&gt; &lt;/c&gt; &lt;c r="E1" s="3" t="s"&gt; &lt;v&gt;10&lt;/v&gt; &lt;/c&gt; &lt;c r="F1" s="3" t="s"&gt; &lt;v&gt;11&lt;/v&gt; &lt;/c&gt; &lt;c r="G1" s="3" t="s"&gt; &lt;v&gt;12&lt;/v&gt; &lt;/c&gt; &lt;c r="H1" s="3" t="s"&gt; &lt;v&gt;13&lt;/v&gt; &lt;/c&gt;&lt;/row&gt; 读取 row c r t 等。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224public class TestEventModel &#123; // EXCEL 读取文件 public void processOneSheet(String filename) throws Exception &#123; OPCPackage pkg = OPCPackage.open(filename); XSSFReader r = new XSSFReader(pkg); SharedStringsTable sst = r.getSharedStringsTable(); XMLReader parser = fetchSheetParser(sst); InputStream sheet = r.getSheet("rId1"); InputSource sheetSource = new InputSource(sheet); parser.parse(sheetSource); sheet.close(); &#125; public XMLReader fetchSheetParser(SharedStringsTable sst) throws SAXException &#123; XMLReader parser = XMLReaderFactory.createXMLReader("org.apache.xerces.parsers.SAXParser"); ContentHandler handler = new SheetHandler(sst); parser.setContentHandler(handler); return parser; &#125; private static class SheetHandler extends DefaultHandler &#123; private SharedStringsTable sst; private String lastContents; private boolean nextIsString; private int curRowNum = 0; private char nextCell = 'A'; private Map&lt;String, String&gt; rowData = new HashMap&lt;String, String&gt;(); private SheetHandler(SharedStringsTable sst) &#123; this.sst = sst; &#125; // 开始读取 excel xml文件中的标签 public void startElement(String uri, String localName, String name, Attributes attributes) throws SAXException &#123; // 读到了一行 &lt;row&gt;&lt;/row&gt; if (name.equals("row")) &#123; // 取得当前的行数 curRowNum = Integer.parseInt(attributes.getValue("r")); // 将下一列数置为第一列，也就是 excel 中的 A nextCell = 'A'; &#125; // 开始读列数 if (name.equals("c")) &#123; // 取得要读取的列数，值为 EXCEL 的单元格位置 A1 B1 C1 D1... String readCell = attributes.getValue("r"); // 取得当前行的第一列 A1 String curCell = nextCell + String.valueOf(curRowNum); // 循环判断，解决单元格空值被跳过的问题（A4为空的话，A4就不会被 attributes.getValue("r") 取到） // 当被跳过，单元格会错位，这里就进行判断，不相等，那么单元格列数向后 + 1 为实际单元格 while (!curCell.equals(readCell)) &#123; // rowData——把一行单元格里的数据整合到一个 Map 中，key 为列数，值为对应的值 rowData.put(curCell, ""); // 单元格列数加一 nextCell = (char) ((int) nextCell + 1); // 取得当前正确的单元格 curCell = nextCell + String.valueOf(curRowNum); &#125; // 由于当前单元格加一，下一个单元格也要做出对应的改变 nextCell = (char) ((int) nextCell + 1); // 处理单元格数据类型，全当做 string 读出来，之后对应处理 String cellType = attributes.getValue("t"); if (cellType != null &amp;&amp; cellType.equals("s")) &#123; nextIsString = true; &#125; else &#123; nextIsString = false; &#125; &#125; lastContents = ""; &#125; public void endElement(String uri, String localName, String name) throws SAXException &#123; // 结束读取 if (nextIsString) &#123; // 取得单元格里的值作为字符串 int idx = Integer.parseInt(lastContents); lastContents = new XSSFRichTextString(sst.getEntryAt(idx)).toString(); nextIsString = false; &#125; if (name.equals("v")) &#123; // 取当前单元格的位置，并作为 key set 到 rowData 中，完成数据封装 String curCell = (char) ((int) nextCell - 1) + String.valueOf(curRowNum); rowData.put(curCell, lastContents); &#125; // 如果读到行结束 if (name.equals("row")) &#123; // 第一行为表头，不处理 if (curRowNum == 1) &#123; return; &#125; // 循环取 A-H 位置的值，这里要取到的值要写死在长度那，超出的不会写入到数据库 for (int i = (int) 'A'; i &lt;= (int) 'H'; i++) &#123; // 取当前列 String curCell = (char) i + String.valueOf(curRowNum); // 从 rowData 里取出值 String data = rowData.get(curCell); // 如果是单元格内是空格，会出现被置为 null 的情况，这里统一置成空字符串 data = data == null ? "" : data.trim(); switch ((char) i) &#123; case 'A': // 如果 A 列是日期，能处理 yyyy/MM/DD 和 yyyy-MM-dd 两种类型 if (data.equals("")) &#123; // do something setDate(null); &#125; else &#123; SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd"); Date date; try &#123; date = sdf.parse(data); &#125; catch (ParseException e) &#123; try &#123; date = calculateDate(data); &#125; catch (Exception e1) &#123; // do something for error message continue; &#125; &#125; // do something setDate(date); &#125; break; case 'B': System.out.printf(d); break; case 'C': System.out.printf(d); break; case 'D': System.out.printf(d); break; case 'E': System.out.printf(d); break; case 'F': System.out.printf(d); break; case 'G': System.out.printf(d); break; case 'H': System.out.printf(d); break; &#125; &#125; rowData.clear(); &#125; // 读完 excel 标志 if (name.equals("worksheet")) &#123; System.out.println("读取结束"); &#125; &#125; public void characters(char[] ch, int start, int length) throws SAXException &#123; lastContents += new String(ch, start, length); &#125; private Date calculateDate(String data) throws Exception &#123; if (null == data) &#123; return null; &#125; // 从 1900年到1970年的天数是25569天，减去之后再进行操作 long millisecond = (Long.parseLong(data) - 25569) * 24 * 60 * 60 * 1000; if (millisecond &lt; 0) &#123; return new Date(0); &#125; return new Date(millisecond); &#125; &#125; public static void main(String[] args) throws Exception &#123; TestEventModel testEventModel = new TestEventModel(); testEventModel.processOneSheet(yourFilePath); &#125;&#125;]]></content>
      <categories>
        <category>软件开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL通识-基础应用]]></title>
    <url>%2F2017%2F12%2F14%2Fmysql-tutorial-1%2F</url>
    <content type="text"><![CDATA[基础运维命令用户管理查询MySQL所有用户：12SELECT User, Host, Password FROM mysql.user;SELECT DISTINCT User FROM mysql.user; 添加MySQL用户：1234create user keaimo identified by 'keaimopw';grant all privileges on *.* to keaimo@'%' identified by 'keaimopw';show grants for 'keaimo'; 忘记root密码：方法一 更改配置文件/etc/my.cnf,在[mysqld]的段上加上一句skip-grant-tables保存并退出； 重启MySQL； 登录并修改密码： 1234mysql&gt; USE mysql ; mysql&gt; UPDATE user SET Password=password( 'new-password' ) WHERE User='root'; mysql&gt; flush privileges ; mysql&gt; quit 将MySQL配置改回来； 重启MySQL； 方法二 KILL掉系统里的MySQL进程； 1killall -TERM mysqld 用以下命令启动MySQL，以不检查权限的方式启动(safe_mysqld是mysqld_safe的符号链接)； 1safe_mysqld --skip-grant-tables &amp; 然后用空密码方式使用root用户登录 MySQL； 1mysql -u root 修改root用户的密码； 123mysql&gt; update mysql.user set password=PASSWORD('新密码') where User='root';mysql&gt; flush privileges;mysql&gt; quit 方法三如果系统中没有safe_mysqld，可以使用如下方式恢复： 停止mysqld(您可能有其它的方法,总之停止mysqld的运行就可以了)； 1/etc/init.d/mysql stop 用以下命令启动MySQL，以不检查权限的方式启动； 1mysqld --skip-grant-tables &amp; 然后用空密码方式使用root用户登录 MySQL； 1mysql -u root 修改root用户的密码； 123mysql&gt; update mysql.user set password=PASSWORD('newpassword') where User='root'; mysql&gt; flush privileges; mysql&gt; quit 重新启动MySQL 1/etc/init.d/mysql restart 环境信息MySQL版本信息 Shell命令： 1mysql -V MySQL中： 1mysql&gt; status; help中查找: 1mysql --help | grep Distrib 使用mysql函数： 1mysql&gt; select version(); 实时SQL监控进入Mysql，启用Log功能(general_log=ON)12SHOW VARIABLES LIKE "general_log%"; SET GLOBAL general_log = 'ON'; 设置Log文件地址(所有Sql语句都会在general_log_file里)1SET GLOBAL general_log_file = 'c:\mysql.log'; 查询结果导出到文件1select count(1) from table into outfile '/data/test.xls'; 123456mysql&gt; pager cat &gt; /tmp/test.txt ;PAGER set to 'cat &gt; /tmp/test.txt'# 之后的所有查询结果都自动写入/tmp/test.txt'，并前后覆盖mysql&gt; select * from table ;30 rows in set (0.59 sec)# 在框口不再显示查询结果]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL通识-SQL]]></title>
    <url>%2F2017%2F12%2F14%2Fmysql-tutorial-3%2F</url>
    <content type="text"></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL通识 - 2]]></title>
    <url>%2F2017%2F12%2F14%2Fmysql-tutorial-2%2F</url>
    <content type="text"><![CDATA[二进制日志准备两台MySQL服务器，版本越接近越好 ### 查看二进制日志状态 123show variables like 'log_bin';show variables like '%log_bin%';show variables like 'binlog_format'; ### 开启二进制日志修改my.cnf，在[mysqld]下面增加log_bin=mysql_bin_log，重启MySQL后，你就会发现log_bin变为了ON。如果在my.cnf里面只设置log_bin，但是不指定file_name，然后重启数据库。你会发现二进制日志文件名称为${hostname}-bin 这样的格式。12[mysqld]log_bin=/mysql/bin_log/mysql_binlog ### 查看二进制日志文件 123show binary logs;show master logs;show master status; 切换二进制日志1flush logs; 每次MySQL重启也会生成新的二进制日志文件。 删除二进制日志purge binary logs to xxx;表示删除某个日志之前的所有二进制日志文件。这个命令会修改index中相关数据：1purge binary logs to 'DB-Server-bin.000002'; 清除某个时间点以前的二进制日志文件：1purge binary logs before '2017-03-10 10:10:00'; 清除7天前的二进制日志文件：1purge master logs before date_sub( now( ), interval 7 day); 清除所有的二进制日志文件（当前不存在主从复制关系）:1reset master; 也可以设置expire_logs_days参数，设置自动清理，其默认值为0,表示不启用过期自动删除功能，如果启用了自动清理功能，表示超出此天数的二进制日志文件将被自动删除，自动删除工作通常发生在MySQL启动时或FLUSH日志时。1show variables like 'expire_logs_days'; 查看二进制日志内容show binlog events使用show binlog events方式可以获取当前以及指定binlog的日志，不适宜提取大量日志。SHOW BINLOG EVENTS[IN &#39;log_name&#39;] [FROM pos] [LIMIT [offset,] row_count]。生产环境不要执行这个命令，可能会导致卡死，应该使用limit限制查询大小：123show binlog events;show binlog events in 'DB-Server-bin.000012';show binlog events in 'DB-Server-bin.000012' from 336; mysqlbinlog当bin-log的模式设置为row时不仅日志长得快 并且查看执行的sql时 也稍微麻烦一点：1.干扰语句多；2生成sql的编码需要解码。直接mysqlbinlog出来的文件执行sql部分的sql显示为base64编码格式，故生成sql记录的时候不能用常规的办法去生成，需要加上相应的参数才能显示出sql语句--base64-output=decode-rows -v。使用时可能会报错mysqlbinlog: unknown variable &#39;default-character-set=utf8mb4&#39;，原因是mysqlbinlog这个工具无法识别binlog中的配置中的default-character-set=utf8这个指令，解决的办法有两个: 将/etc/my.cnf中配置的default-character-set = utf8mb4修改为character-set-server = utf8mb4 但是这种修改方法需要重启数据库, 在线上业务库中使用这种方法查看 binlog 日志并不划算 mysqlbinlog –no-defaults mysql-bin.000256 完美解决; 123456789101112131415161718192021222324252627# 提取指定的binlog日志 mysqlbinlog /opt/data/APP01bin.000001 mysqlbinlog /opt/data/APP01bin.000001|grep insert # 提取指定position位置的binlog日志 mysqlbinlog --start-position="120" --stop-position="332" /opt/data/APP01bin.000001 # 提取指定position位置的binlog日志并输出到压缩文件 mysqlbinlog --start-position="120" --stop-position="332" /opt/data/APP01bin.000001 |gzip &gt;extra_01.sql.gz # 提取指定position位置的binlog日志导入数据库 mysqlbinlog --start-position="120" --stop-position="332" /opt/data/APP01bin.000001 | mysql -uroot -p # 提取指定开始时间的binlog并输出到日志文件 mysqlbinlog --start-datetime="2014-12-15 20:15:23" /opt/data/APP01bin.000002 --result-file=extra02.sql # 提取指定位置的多个binlog日志文件 mysqlbinlog --start-position="120" --stop-position="332" /opt/data/APP01bin.000001 /opt/data/APP01bin.000002|more # 提取指定数据库binlog并转换字符集到UTF8 mysqlbinlog --database=test --set-charset=utf8 /opt/data/APP01bin.000001 /opt/data/APP01bin.000002 &gt;test.sql # 远程提取日志，指定结束时间 mysqlbinlog -urobin -p -h192.168.1.116 -P3306 --stop-datetime="2014-12-15 20:30:23" --read-from-remote-server mysql-bin.000033 |more # 远程提取使用row格式的binlog日志并输出到本地文件 mysqlbinlog -urobin -p -P3606 -h192.168.1.177 --read-from-remote-server -vv inst3606bin.000005 &gt;row.sql]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux内存中的Cache]]></title>
    <url>%2F2017%2F12%2F12%2Flinux-cache%2F</url>
    <content type="text"><![CDATA[在Linux系统中，我们经常用free命令来查看系统内存的使用状态。在一个RHEL6的系统上，free命令的显示内容大概是这样一个状态：12345[root@tencent64 ~]# free total used free shared buffers cachedMem: 132256952 72571772 59685180 0 1762632 53034704-/+ buffers/cache: 17774436 114482516Swap: 2101192 508 2100684 这里的默认显示单位是kb，我的服务器是128G内存，所以数字显得比较大。这个命令几乎是每一个使用过Linux的人必会的命令，但越是这样的命令，似乎真正明白的人越少（我是说比例越少）。一般情况下，对此命令输出的理解可以分这几个层次： 不了解。这样的人的第一反应是：天啊，内存用了好多，70个多G，可是我几乎没有运行什么大程序啊？为什么会这样？Linux好占内存！ 自以为很了解。这样的人一般评估过会说：嗯，根据我专业的眼光看的出来，内存才用了17G左右，还有很多剩余内存可用。buffers/cache占用的较多，说明系统中有进程曾经读写过文件，但是不要紧，这部分内存是当空闲来用的。 真的很了解。这种人的反应反而让人感觉最不懂Linux，他们的反应是：free显示的是这样，好吧我知道了。神马？你问我这些内存够不够，我当然不知道啦！我特么怎么知道你程序怎么写的？ 根据目前网络上技术文档的内容，我相信绝大多数了解一点Linux的人应该处在第二种层次。大家普遍认为，buffers和cached所占用的内存空间是可以在内存压力较大的时候被释放当做空闲空间用的。但真的是这样么？在论证这个题目之前，我们先简要介绍一下buffers和cached是什么意思： 什么是buffer/cachebuffer和cache是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义。在Linux的内存管理中，这里的buffer指Linux内存的：Buffer cache。这里的cache指Linux内存中的：Page cache。翻译成中文可以叫做缓冲区缓存和页面缓存。在历史上，它们一个（buffer）被用来当成对io设备写的缓存，而另一个（cache）被用来当作对io设备的读缓存，这里的io设备，主要指的是块设备文件和文件系统上的普通文件。但是现在，它们的意义已经不一样了。在当前的内核中，page cache顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以page进行分配管理的，都可以使用page cache作为其缓存来管理使用。当然，不是所有的内存都是以页（page）进行管理的，也有很多是针对块（block）进行管理的，这部分内存使用如果要用到cache功能，则都集中到buffer cache中来使用。（从这个角度出发，是不是buffer cache改名叫做block cache更好？）然而，也不是所有块（block）都有固定长度，系统上块的长度主要是根据所使用的块设备决定的，而页长度在X86上无论是32位还是64位都是4k。 明白了这两套缓存系统的区别，就可以理解它们究竟都可以用来做什么了。 什么是page cachePage cache主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有read／write操作的时候。如果你仔细想想的话，作为可以映射文件到内存的系统调用：mmap是不是很自然的也应该用到page cache？在当前的系统实现里，page cache也被作为其它文件类型的缓存设备来用，所以事实上page cache也负责了大部分的块设备文件的缓存工作。 什么是buffer cacheBuffer cache则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。这意味着某些对块的操作会使用buffer cache进行缓存，比如我们在格式化文件系统的时候。一般情况下两个缓存系统是一起配合使用的，比如当我们对一个文件进行写操作的时候，page cache的内容会被改变，而buffer cache则可以用来将page标记为不同的缓冲区，并记录是哪一个缓冲区被修改了。这样，内核在后续执行脏数据的回写（writeback）时，就不用将整个page写回，而只需要写回修改的部分即可。 如何回收cacheLinux内核会在内存将要耗尽的时候，触发内存回收的工作，以便释放出内存给急需内存的进程使用。一般情况下，这个操作中主要的内存释放都来自于对buffer／cache的释放。尤其是被使用更多的cache空间。既然它主要用来做缓存，只是在内存够用的时候加快进程对文件的读写速度，那么在内存压力较大的情况下，当然有必要清空释放cache，作为free空间分给相关进程使用。所以一般情况下，我们认为buffer/cache空间可以被释放，这个理解是正确的。 但是这种清缓存的工作也并不是没有成本。理解cache是干什么的就可以明白清缓存必须保证cache中的数据跟对应文件中的数据一致，才能对cache进行释放。所以伴随着cache清除的行为的，一般都是系统IO飙高。因为内核要对比cache中的数据和对应硬盘文件上的数据是否一致，如果不一致需要写回，之后才能回收。 在系统中除了内存将被耗尽的时候可以清缓存以外，我们还可以使用下面这个文件来人工触发缓存清除的操作：12[root@tencent64 ~]# cat /proc/sys/vm/drop_caches 1 方法是：1echo 1 &gt; /proc/sys/vm/drop_caches 当然，这个文件可以设置的值分别为1、2、3。它们所表示的含义为：echo 1 &gt; /proc/sys/vm/drop_caches:表示清除pagecache。 echo 2 &gt; /proc/sys/vm/drop_caches:表示清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。 echo 3 &gt; /proc/sys/vm/drop_caches:表示清除pagecache和slab分配器中的缓存对象。 cache都能被回收么我们分析了cache能被回收的情况，那么有没有不能被回收的cache呢？当然有。我们先来看第一种情况： tmpfs大家知道Linux提供一种“临时”文件系统叫做tmpfs，它可以将内存的一部分空间拿来当做文件系统使用，使内存空间可以当做目录文件来用。现在绝大多数Linux系统都有一个叫做/dev/shm的tmpfs目录，就是这样一种存在。当然，我们也可以手工创建一个自己的tmpfs，方法如下： 12345678910[root@tencent64 ~]# mkdir /tmp/tmpfs[root@tencent64 ~]# mount -t tmpfs -o size=20G none /tmp/tmpfs/[root@tencent64 ~]# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/sda1 10325000 3529604 6270916 37% //dev/sda3 20646064 9595940 10001360 49% /usr/local/dev/mapper/vg-data 103212320 26244284 71725156 27% /datatmpfs 66128476 14709004 51419472 23% /dev/shmnone 20971520 0 20971520 0% /tmp/tmpfs 于是我们就创建了一个新的tmpfs，空间是20G，我们可以在/tmp/tmpfs中创建一个20G以内的文件。如果我们创建的文件实际占用的空间是内存的话，那么这些数据应该占用内存空间的什么部分呢？根据pagecache的实现功能可以理解，既然是某种文件系统，那么自然该使用pagecache的空间来管理。我们试试是不是这样？123456789101112131415[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 36 89 0 1 19-/+ buffers/cache: 15 111Swap: 2 0 2[root@tencent64 ~]# dd if=/dev/zero of=/tmp/tmpfs/testfile bs=1G count=1313+0 records in13+0 records out13958643712 bytes (14 GB) copied, 9.49858 s, 1.5 GB/s[root@tencent64 ~]# [root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 49 76 0 1 32-/+ buffers/cache: 15 110Swap: 2 0 2 我们在tmpfs目录下创建了一个13G的文件，并通过前后free命令的对比发现，cached增长了13G，说明这个文件确实放在了内存里并且内核使用的是cache作为存储。再看看我们关心的指标： -/+ buffers/cache那一行。我们发现，在这种情况下free命令仍然提示我们有110G内存可用，但是真的有这么多么？我们可以人工触发内存回收看看现在到底能回收多少内存：123456[root@tencent64 ~]# echo 3 &gt; /proc/sys/vm/drop_caches[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 43 82 0 0 29-/+ buffers/cache: 14 111Swap: 2 0 2 可以看到，cached占用的空间并没有像我们想象的那样完全被释放，其中13G的空间仍然被/tmp/tmpfs中的文件占用的。当然，我的系统中还有其他不可释放的cache占用着其余16G内存空间。那么tmpfs占用的cache空间什么时候会被释放呢？是在其文件被删除的时候.如果不删除文件，无论内存耗尽到什么程度，内核都不会自动帮你把tmpfs中的文件删除来释放cache空间。 123456[root@tencent64 ~]# rm /tmp/tmpfs/testfile [root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 30 95 0 0 16-/+ buffers/cache: 14 111Swap: 2 0 2 这是我们分析的第一种cache不能被回收的情况。还有其他情况，比如： 共享内存共享内存是系统提供给我们的一种常用的进程间通信（IPC）方式，但是这种通信方式不能在shell中申请和使用，所以我们需要一个简单的测试程序，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[root@tencent64 ~]# cat shm.c #include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/ipc.h&gt;#include &lt;sys/shm.h&gt;#include &lt;string.h&gt;#define MEMSIZE 2048*1024*1023intmain()&#123; int shmid; char *ptr; pid_t pid; struct shmid_ds buf; int ret; shmid = shmget(IPC_PRIVATE, MEMSIZE, 0600); if (shmid&lt;0) &#123; perror("shmget()"); exit(1); &#125; ret = shmctl(shmid, IPC_STAT, &amp;buf); if (ret &lt; 0) &#123; perror("shmctl()"); exit(1); &#125; printf("shmid: %d\n", shmid); printf("shmsize: %d\n", buf.shm_segsz); buf.shm_segsz *= 2; ret = shmctl(shmid, IPC_SET, &amp;buf); if (ret &lt; 0) &#123; perror("shmctl()"); exit(1); &#125; ret = shmctl(shmid, IPC_SET, &amp;buf); if (ret &lt; 0) &#123; perror("shmctl()"); exit(1); &#125; printf("shmid: %d\n", shmid); printf("shmsize: %d\n", buf.shm_segsz); pid = fork(); if (pid&lt;0) &#123; perror("fork()"); exit(1); &#125; if (pid==0) &#123; ptr = shmat(shmid, NULL, 0); if (ptr==(void*)-1) &#123; perror("shmat()"); exit(1); &#125; bzero(ptr, MEMSIZE); strcpy(ptr, "Hello!"); exit(0); &#125; else &#123; wait(NULL); ptr = shmat(shmid, NULL, 0); if (ptr==(void*)-1) &#123; perror("shmat()"); exit(1); &#125; puts(ptr); exit(0); &#125;&#125; 程序功能很简单，就是申请一段不到2G共享内存，然后打开一个子进程对这段共享内存做一个初始化操作，父进程等子进程初始化完之后输出一下共享内存的内容，然后退出。但是退出之前并没有删除这段共享内存。我们来看看这个程序执行前后的内存使用： 12345678910111213141516[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 30 95 0 0 16-/+ buffers/cache: 14 111Swap: 2 0 2[root@tencent64 ~]# ./shm shmid: 294918shmsize: 2145386496shmid: 294918shmsize: -4194304Hello![root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 32 93 0 0 18-/+ buffers/cache: 14 111Swap: 2 0 2 cached空间由16G涨到了18G。那么这段cache能被回收么？继续测试： 123456[root@tencent64 ~]# echo 3 &gt; /proc/sys/vm/drop_caches[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 32 93 0 0 18-/+ buffers/cache: 14 111Swap: 2 0 2 结果是仍然不可回收。大家可以观察到，这段共享内存即使没人使用，仍然会长期存放在cache中，直到其被删除。删除方法有两种，一种是程序中使用shmctl()去IPC_RMID，另一种是使用ipcrm命令。我们来删除试试： 1234567891011121314151617181920212223242526272829[root@tencent64 ~]# ipcs -m------ Shared Memory Segments --------key shmid owner perms bytes nattch status 0x00005feb 0 root 666 12000 4 0x00005fe7 32769 root 666 524288 2 0x00005fe8 65538 root 666 2097152 2 0x00038c0e 131075 root 777 2072 1 0x00038c14 163844 root 777 5603392 0 0x00038c09 196613 root 777 221248 0 0x00000000 294918 root 600 2145386496 0 [root@tencent64 ~]# ipcrm -m 294918[root@tencent64 ~]# ipcs -m------ Shared Memory Segments --------key shmid owner perms bytes nattch status 0x00005feb 0 root 666 12000 4 0x00005fe7 32769 root 666 524288 2 0x00005fe8 65538 root 666 2097152 2 0x00038c0e 131075 root 777 2072 1 0x00038c14 163844 root 777 5603392 0 0x00038c09 196613 root 777 221248 0 [root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 30 95 0 0 16-/+ buffers/cache: 14 111Swap: 2 0 2 删除共享内存后，cache被正常释放了。这个行为与tmpfs的逻辑类似。内核底层在实现共享内存（shm）、消息队列（msg）和信号量数组（sem）这些POSIX:XSI的IPC机制的内存存储时，使用的都是tmpfs。这也是为什么共享内存的操作逻辑与tmpfs类似的原因。当然，一般情况下是shm占用的内存更多，所以我们在此重点强调共享内存的使用。说到共享内存，Linux还给我们提供了另外一种共享内存的方法，就是： mmapmmap()是一个非常重要的系统调用，这仅从mmap本身的功能描述上是看不出来的。从字面上看，mmap就是将一个文件映射进进程的虚拟内存地址，之后就可以通过操作内存的方式对文件的内容进行操作。但是实际上这个调用的用途是很广泛的。当malloc申请内存时，小段内存内核使用sbrk处理，而大段内存就会使用mmap。当系统调用exec族函数执行时，因为其本质上是将一个可执行文件加载到内存执行，所以内核很自然的就可以使用mmap方式进行处理。我们在此仅仅考虑一种情况，就是使用mmap进行共享内存的申请时，会不会跟shmget()一样也使用cache？ 同样，我们也需要一个简单的测试程序：12345678910111213141516171819202122232425262728293031323334353637383940[root@tencent64 ~]# cat mmap.c #include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;strings.h&gt;#include &lt;sys/mman.h&gt;#include &lt;sys/stat.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#define MEMSIZE 1024*1024*1023*2#define MPFILE "./mmapfile"int main()&#123; void *ptr; int fd; fd = open(MPFILE, O_RDWR); if (fd &lt; 0) &#123; perror("open()"); exit(1); &#125; ptr = mmap(NULL, MEMSIZE, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_ANON, fd, 0); if (ptr == NULL) &#123; perror("malloc()"); exit(1); &#125; printf("%p\n", ptr); bzero(ptr, MEMSIZE); sleep(100); munmap(ptr, MEMSIZE); close(fd); exit(1);&#125; 这次我们干脆不用什么父子进程的方式了，就一个进程，申请一段2G的mmap共享内存，然后初始化这段空间之后等待100秒，再解除影射所以我们需要在它sleep这100秒内检查我们的系统内存使用，看看它用的是什么空间？当然在这之前要先创建一个2G的文件./mmapfile。结果如下：1234567[root@tencent64 ~]# dd if=/dev/zero of=mmapfile bs=1G count=2[root@tencent64 ~]# echo 3 &gt; /proc/sys/vm/drop_caches[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 30 95 0 0 16-/+ buffers/cache: 14 111Swap: 2 0 2 然后执行测试程序： 123456789101112131415[root@tencent64 ~]# ./mmap &amp;[1] 191570x7f1ae3635000[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 32 93 0 0 18-/+ buffers/cache: 14 111Swap: 2 0 2[root@tencent64 ~]# echo 3 &gt; /proc/sys/vm/drop_caches[root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 32 93 0 0 18-/+ buffers/cache: 14 111Swap: 2 0 2 我们可以看到，在程序执行期间，cached一直为18G，比之前涨了2G，并且此时这段cache仍然无法被回收。然后我们等待100秒之后程序结束。 12345678[root@tencent64 ~]# [1]+ Exit 1 ./mmap[root@tencent64 ~]# [root@tencent64 ~]# free -g total used free shared buffers cachedMem: 126 30 95 0 0 16-/+ buffers/cache: 14 111Swap: 2 0 2 程序退出之后，cached占用的空间被释放。这样我们可以看到，使用mmap申请标志状态为MAP_SHARED的内存，内核也是使用的cache进行存储的。在进程对相关内存没有释放之前，这段cache也是不能被正常释放的。实际上，mmap的MAP_SHARED方式申请的内存，在内核中也是由tmpfs实现的。由此我们也可以推测，由于共享库的只读部分在内存中都是以mmap的MAP_SHARED方式进行管理，实际上它们也都是要占用cache且无法被释放的。 最后我们通过三个测试例子，发现Linux系统内存中的cache并不是在所有情况下都能被释放当做空闲空间用的。并且也也明确了，即使可以释放cache，也并不是对系统来说没有成本的。总结一下要点，我们应该记得这样几点： 当cache作为文件缓存被释放的时候会引发IO变高，这是cache加快文件访问速度所要付出的成本。 tmpfs中存储的文件会占用cache空间，除非文件删除否则这个cache不会被自动释放。 使用shmget方式申请的共享内存会占用cache空间，除非共享内存被ipcrm或者使用shmctl去IPC_RMID，否则相关的cache空间都不会被自动释放。 使用mmap方法申请的MAP_SHARED标志的内存会占用cache空间，除非进程将这段内存munmap，否则相关的cache空间都不会被自动释放。 实际上shmget、mmap的共享内存，在内核层都是通过tmpfs实现的，tmpfs实现的存储用的都是cache。 当理解了这些的时候，希望大家对free命令的理解可以达到我们说的第三个层次。我们应该明白，内存的使用并不是简单的概念，cache也并不是真的可以当成空闲空间用的。如果我们要真正深刻理解你的系统上的内存到底使用的是否合理，是需要理解清楚很多更细节知识，并且对相关业务的实现做更细节判断的。我们当前实验场景是Centos 6的环境，不同版本的Linux的free现实的状态可能不一样，大家可以自己去找出不同的原因。 当然，本文所述的也不是所有的cache不能被释放的情形。那么，在你的应用场景下，还有那些cache不能被释放的场景呢？]]></content>
      <categories>
        <category>Linux运维</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java关键字通识]]></title>
    <url>%2F2017%2F12%2F11%2Fjava-keywords%2F</url>
    <content type="text"><![CDATA[strictfpstrictfp的意思是FP-strict，也就是说精确浮点的意思。在Java虚拟机进行浮点运算时，如果没有指定strictfp关键字时，Java的编译器以及运行环境在对浮点运算的表达式是采取一种近似于我行我素的行为来完成这些操作，以致于得到的结果往往无法令你满意。而一旦使用了strictfp来声明一个类、接口或者方法时，那么所声明的范围内Java的编译器以及运行环境会完全依照浮点规范IEEE-754来执行。因此如果你想让你的浮点运算更加精确，而且不会因为不同的硬件平台所执行的结果不一致的话，那就请用关键字strictfp。 strictfp不能解决所谓”精确“问题，是用来保证可移植性的。如果没有加strictfp关键字，在不同的平台下面，可能会得到不同的结果。使用strictfp关键字的目的，是保证平台移植之后，浮点运算结果是一致的。如果要进行精确的金额计算，还是需要使用BigDecimal。 transient变量修饰符(只能修饰字段)。标记为transient的变量，在对象存储时，这些变量状态不会被持久化。当对象序列化的保存在存储器上时，不希望有些字段数据被保存，为了保证安全性，可以把这些字段声明为transient。 volatilevolatile修饰变量。在每次被线程访问时，都强迫从共享内存中重读该成员变量的值。而且，当成员变量发生变化时，强迫线程将变化值回写到共享内存。这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值。 Java内存模型规定了所有变量都存储在主内存中，每条线程都有自己的工作内存，线程的工作内存保存了被该线程使用到变量的主内存副本拷贝，线程对变量的所有操作(读取,赋值等)都必须在工作内存中进行，而不能直接读写主内存中的变量。不同线程也不能直接访问对方工作内存中的变量，线程间变量值的传递均需要通过主内存来完成。 当一个变量定义成volatile之后, 保证了此变量对所有线程的可见性,也就是说当一条线程修改了这个变量的值,新的值对于其它线程来说是可以立即得知的.此时,该变量的读写操作直接在主内存中完成。Volatile 变量具有 synchronized 的可见性特性，但是不具备原子特性。虽然增量操作（x++）看上去类似一个单独操作，实际上它是一个由读取－修改－写入操作序列组成的组合操作，必须以原子方式执行，而 volatile 不能提供必须的原子特性。 在多线程并发的环境下, 各个线程的读/写操作可能有重叠现象, 在这个时候, volatile并不能保证数据同步。 volatile适用于简单的标志量的场景中。如：12345678910public class CheesyCounter &#123; private volatile int value; public int getValue() &#123; return value; &#125; public synchronized int increment() &#123; return value++; &#125;&#125; synchronized通过 synchronized 关键字来实现，所有加上synchronized 和 块语句，在多线程访问的时候，同一时刻只能有一个线程能够用。synchronized用来修饰方法或者代码块。]]></content>
      <categories>
        <category>软件开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java多线程常识]]></title>
    <url>%2F2017%2F12%2F09%2Fjava-multithreading-common-sense%2F</url>
    <content type="text"><![CDATA[线程安全给线程安全下定义比较困难。存在很多种定义，如：“一个类在可以被多个线程安全调用时就是线程安全的”。 静态变量使用static关键字定义的变量。static可以修饰变量和方法，也有static静态代码块。被static修饰的成员变量和成员方法独立于该类的任何对象。也就是说，它不依赖类特定的实例，被类的所有实例共享。只要这个类被加载，Java虚拟机就能根据类名在运行时数据区的方法区内定找到他们。因此，static对象可以在它的任何对象创建之前访问，无需引用任何对象。静态变量通常用于对象间共享值、方便访问变量等场景。静态变量即类变量，位于方法区，为所有对象共享，共享一份内存，一旦静态变量被修改，其他对象均对修改可见，静态变量是非线程安全的。 实例变量单例模式（只有一个对象实例存在）线程非安全，非单例线程安全。 实例变量为对象实例私有，在虚拟机的堆中分配，若在系统中只存在一个此对象的实例，在多线程环境下，“犹如”静态变量那样，被某个线程修改后，其他线程对修改均可见，故线程非安全；如果每个线程执行都是在不同的对象中，那对象与对象之间的实例变量的修改将互不影响，故线程安全。 局部变量线程安全。每个线程执行时将会把局部变量放在各自栈帧的工作内存中，线程间不共享，故不存在线程安全问题。 线程安全的单例模式单例模式是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例类的特殊类。通过单例模式可以保证系统中一个类只有一个实例而且该实例易于外界访问，从而方便对实例个数的控制并节约系统资源。如果希望在系统中某个类的对象只能存在一个，单例模式是最好的解决方案。 Double Check在 Effecitve Java 一书的第 48 条中提到了双重检查模式，并指出这种模式在 Java 中通常并不适用。该模式的结构如下所示：12345678910public Resource getResource() &#123; if (resource == null) &#123; synchronized(this)&#123; if (resource==null) &#123; resource = new Resource(); &#125; &#125; &#125; return resource;&#125; 该模式是对下面的代码改进：123456public synchronized Resource getResource()&#123; if (resource == null)&#123; resource = new Resource(); &#125; return resource;&#125; 这段代码的目的是对 resource 延迟初始化。但是每次访问的时候都需要同步。为了减少同步的开销，于是有了双重检查模式。在 Java 中双重检查模式无效的原因是在不同步的情况下引用类型不是线程安全的。对于除了 long 和 double 的基本类型，双重检查模式是适用 的。比如下面这段代码就是正确的：1234567891011private int count;public int getCount()&#123; if (count == 0)&#123; synchronized(this)&#123; if (count == 0)&#123; count = computeCount(); //一个耗时的计算 &#125; &#125; &#125; return count;&#125; 上面就是关于java中双重检查模式（double-check idiom）的一般结论。但是事情还没有结束，因为java的内存模式也在改进中。Doug Lea 在他的文章中写道：“根据最新的 JSR133 的 Java 内存模型，如果将引用类型声明为 volatile，双重检查模式就可以工作了”，参见 http://gee.cs.oswego.edu/dl/cpj/updates.html 。所以以后要在 Java 中使用双重检查模式，可以使用下面的代码：1234567891011private volatile Resource resource;public Resource getResource()&#123; if (resource == null)&#123; synchronized(this)&#123; if (resource==null)&#123; resource = new Resource(); &#125; &#125; &#125; return resource;&#125; 当然了，得是在遵循 JSR133 规范的 Java 中。所以，double-check 在 J2SE 1.4 或早期版本在多线程或者 JVM 调优时由于 out-of-order writes，是不可用的。 这个问题在 J2SE 5.0 中已经被修复，可以使用 volatile 关键字来保证多线程下的单例。12345678910111213public class Singleton &#123; private volatile Singleton instance = null; public Singleton getInstance() &#123; if (instance == null) &#123; synchronized(this) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; IODH推荐方法 是Initialization on Demand Holder（IODH），详见 http://en.wikipedia.org/wiki/Initialization_on_demand_holder_idiom12345678910public class Singleton &#123; static class SingletonHolder &#123; static Singleton instance = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonHolder.instance; &#125;&#125; 编译并运行上述代码，运行结果为：true，即创建的单例对象s1和s2为同一对象。由于静态单例对象没有作为Singleton的成员变量直接实例化，因此类加载时不会实例化Singleton，第一次调用getInstance()时将加载内部类SingletonHolder，在该内部类中定义了一个static类型的变量instance，此时会首先初始化这个成员变量，由Java虚拟机来保证其线程安全性，确保该成员变量只能初始化一次。由于getInstance()方法没有任何线程锁定，因此其性能不会造成任何影响。通过使用IoDH，我们既可以实现延迟加载，又可以保证线程安全，不影响系统性能，不失为一种最好的Java语言单例模式实现方式（其缺点是与编程语言本身的特性相关，很多面向对象语言不支持IoDH）。 线程池Java通过Executors提供四种线程池，分别为：newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 ExecutorServiceExecutorServcie中执行一个Runnable有两个方法，两个分别是：12public void execute(Runnable command);public &lt;T&gt; Future&lt;T&gt; submit(Runnable task, T result); 其实submit最后是调用的execute的，而且在调用execute前，对task进行了一次封装，变成了RunnableFuture。 executesubmit创建线程池newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。示例代码如下：123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cachedThreadPool.execute(new Runnable() &#123; public void run() &#123; System.out.println(index); &#125; &#125;); &#125; &#125; &#125; 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。 newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。示例代码如下：123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125; 因为线程池大小为3，每个任务输出index后sleep 2秒，所以每两秒打印3个数字。定长线程池的大小最好根据系统资源进行设置。如Runtime.getRuntime().availableProcessors() newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。延迟执行示例代码如下：1234567891011121314package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.schedule(new Runnable() &#123; public void run() &#123; System.out.println("delay 3 seconds"); &#125; &#125;, 3, TimeUnit.SECONDS); &#125; &#125; 表示延迟3秒执行。定期执行示例代码如下：1234567891011121314package test; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123; public void run() &#123; System.out.println("delay 1 seconds, and excute every 3 seconds"); &#125; &#125;, 1, 3, TimeUnit.SECONDS); &#125; &#125; 表示延迟1秒后每3秒执行一次。 newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。示例代码如下：123456789101112131415161718192021package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; public void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; &#125; &#125; 结果依次输出，相当于顺序执行各个任务。你可以使用JDK自带的监控工具来监控我们创建的线程数量，运行一个不终止的线程，创建指定量的线程，来观察：工具目录：C:\Program Files\Java\jdk1.6.0_06\bin\jconsole.exe运行程序做稍微修改：12345678910111213141516171819202122232425262728package test; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class ThreadPoolExecutorTest &#123; public static void main(String[] args) &#123; ExecutorService singleThreadExecutor = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; singleThreadExecutor.execute(new Runnable() &#123; public void run() &#123; try &#123; while(true) &#123; System.out.println(index); Thread.sleep(10 * 1000); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;]]></content>
      <categories>
        <category>软件开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java内存分析]]></title>
    <url>%2F2017%2F12%2F08%2Fjava-memory-analysis%2F</url>
    <content type="text"><![CDATA[内存分析通过top命令定位占用大内存的应用，通过jps命令找到应用进程号。打印出某个java进程（使用pid）内存内的，所有‘对象’的情况（如：产生那些对象，及其数量）。 jmapjmap命令(Java Memory Map) jmap -heap [pid] 查看整个JVM内存状态; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Attaching to process ID 22999, please wait...Debugger attached successfully.Server compiler detected.JVM version is 24.80-b11using thread-local object allocation.Parallel GC with 4 thread(s)Heap Configuration: MinHeapFreeRatio = 0 MaxHeapFreeRatio = 100 MaxHeapSize = 2065694720 (1970.0MB) NewSize = 1310720 (1.25MB) MaxNewSize = 17592186044415 MB OldSize = 5439488 (5.1875MB) NewRatio = 2 SurvivorRatio = 8 PermSize = 21757952 (20.75MB) MaxPermSize = 85983232 (82.0MB) G1HeapRegionSize = 0 (0.0MB)Heap Usage:PS Young GenerationEden Space: capacity = 24117248 (23.0MB) used = 8397088 (8.008087158203125MB) free = 15720160 (14.991912841796875MB) 34.81777025305706% usedFrom Space: capacity = 524288 (0.5MB) used = 98304 (0.09375MB) free = 425984 (0.40625MB) 18.75% usedTo Space: capacity = 524288 (0.5MB) used = 0 (0.0MB) free = 524288 (0.5MB) 0.0% usedPS Old Generation capacity = 85983232 (82.0MB) used = 44148184 (42.102989196777344MB) free = 41835048 (39.897010803222656MB) 51.34510877655774% usedPS Perm Generation capacity = 39321600 (37.5MB) used = 39033120 (37.224884033203125MB) free = 288480 (0.275115966796875MB) 99.266357421875% used19166 interned Strings occupying 1688912 bytes. jmap -histo [pid] 查看JVM堆中对象详细占用情况 1234567891011121314151617num #instances #bytes class name---------------------------------------------- 1: 10823 3072312 [B 2: 16605 2318720 &lt;constMethodKlass&gt; 3: 18687 1388088 [C 4: 16605 1328608 &lt;methodKlass&gt; 5: 27595 1296832 &lt;symbolKlass&gt; 6: 1699 940392 &lt;constantPoolKlass&gt; 7: 2520 883408 [I 8: 1699 724944 &lt;instanceKlassKlass&gt; 9: 1472 565136 &lt;constantPoolCacheKlass&gt;10: 256 561152 [Lnet.sf.ehcache.store.chm.SelectableConcurrentHashMap$HashEntry;11: 12148 291552 java.lang.String12: 4505 288320 net.sf.ehcache.Element13: 7290 233280 java.lang.ThreadLocal$ThreadLocalMap$Entry14: 1946 186816 java.lang.Class15: 4509 180360 net.sf.ehcache.store.chm.SelectableConcurrentHashMap$HashEntry 其中[开头表示数组，[C [I [B 分别是char[] int[] byte[]。constMethodKlass、都实现自sun.jvm.hotspot.oops.Klass，用于在永久代里保存类的信息。 jmap -dump:format=b,file=文件名 [pid] 导出整个JVM 中内存信息,通过jhat启动一个Web Server（端口7000）查看分析结果：12jmap -dump:file=a.txt 22999jhat -J-Xmx512m a.txt kill -3 [pid]在Linux 上找到Java所在的进程号，然后执行以上命令，线程的相关信息就输出到console。inux的kill -3指令可以帮我们输出当前进程中所有线程的状态，如哪些线程在运行，哪些在等待，因为什么等待，代码哪一行等待。kill -3会将信息输出至控制台，所以使用时，被kill -3的进程最好是nohup启动的。kill -3并不会影响程序运行，不用担心他把程序杀死了。PS: 需要-Xrs JVM选择被使用。 jstackjstack 是sun JDK 自带的工具，通过该工具可以看到JVM 中线程的运行状况，包括锁等待，线程是否在运行。]]></content>
      <categories>
        <category>软件开发</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux运维基础]]></title>
    <url>%2F2017%2F12%2F08%2Flinux-ops-base%2F</url>
    <content type="text"><![CDATA[磁盘管理查看文件(夹)所在分区(挂载点)1234df /pathcat /etc/mtabmountfdisk -l 查找大文件/文件夹：1234find . -type f -size +800Mfind . -type f -size +800M -print0 | xargs -0 ls -lfind . -type f -size +800M -print0 | xargs -0 du -hfind . -type f -size +800M -print0 | xargs -0 du -h | sort -nr 运行状态TOP查看系统运行情况： 输入大写P，结果按CPU占用降序排序； 输入大写M，结果按内存占用降序排序； 网络安全当VPS暴露在外网中，就会有人不断暴力破解你的SSH登录。于是就有必要使用SSH密钥来登录。并关闭密码登录。用以下命令可以查看别人暴力破解你SSH密码登录的大概情况：1grep "Failed password for invalid user" /var/log/secure | awk '&#123;print $13&#125;' | sort | uniq -c | sort -nr | more 这时可以使用SSH秘钥登录来防止暴力破解。]]></content>
      <categories>
        <category>Linux运维</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[iframe无刷新跨域上传文件并获取返回值]]></title>
    <url>%2F2017%2F12%2F08%2Fweb-upfile-iframe%2F</url>
    <content type="text"><![CDATA[通常我们会有一个统一的上传接口，这个接口会被其他的服务调用。如果出现不同域，还需要无刷新上传文件，并且获取返回值，这就有点麻烦了。比如，新浪微博启用了新域名www.weibo.com，但接口还是使用原来的域：picupload.t.sina.com.cn。 研究了一下新浪微博的处理方法，这里大概演示一下。 首先是一个正常的上传页面 upload.html 123456789101112131415&lt;script&gt; // 这个函数将来会被iframe用到 function getIframeVal(val) &#123; alert(val); &#125;&lt;/script&gt;&lt;!-- 我把upload.com指向了127.0.0.1 --&gt;&lt;form method="post" target="if" enctype="multipart/form-data" action="http://upload.com/playground/js/deal.php?cb=http://localhost/playground/js/deal_cd.html"&gt; &lt;input type="file" name="file" /&gt; &lt;input type="SUBMIT" value="upload" /&gt;&lt;/form&gt;&lt;IFRAME id="if" name="if" src="about:blank" frameborder='0'&gt;&lt;/IFRAME&gt; 这里有一个关键点是form的target要指向iframe，同时把iframe隐藏起来，这样上传的处理结果就会显示在该iframe里。action里的cb(callback)参数表示处理完成后要跳转的url，因为我们的目标是iframe，所以只会把跳转的页面输出到iframe，而不会让当前页面跳转。 还有一点，callback url要和当前页面同域。跨域的iframe无法调用父页面的内容。 再来看看deal.php，也就是form的action 1234&lt;?php// deal upload file// and get file id, you can pass other params eitherheader('location:'.$_GET['cb'].'?file_id=123'); 这里可以处理文件，然后入库。操作完成后，把文件的id及其他信息都放在url里，最后跳转到这个url。 最后来看看deal_cd.html，也就是刚刚deal.php跳转到的url，这个文件的内容会填充到页面的iframe里。 1234&lt;script type="text/javascript"&gt; var rs = window.location.search.split('?').slice(1); window.parent.getIframeVal(rs.toString().split('=').slice(1));&lt;/script&gt; 这里调用了父窗口的getIframeVal方法，这样父页面就获得了文件的id。]]></content>
      <tags>
        <tag>软件开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[find命令-print0和xargs中-0的奥妙]]></title>
    <url>%2F2017%2F12%2F08%2Flinux-find-xargs%2F</url>
    <content type="text"><![CDATA[-print0默认情况下, find 每输出一个文件名, 后面都会接着输出一个换行符 (‘\n’), 因此我们看到的 find 的输出都是一行一行的: 12345678[bash-4.1.5] ; ls -ltotal 0-rw-r--r-- 1 root root 0 2010-08-02 18:09 file1.log-rw-r--r-- 1 root root 0 2010-08-02 18:09 file2.log[bash-4.1.5] ; find -name '*.log'./file2.log./file1.log 比如我想把所有的 .log 文件删掉, 可以这样配合 xargs 一起用:12345[bash-4.1.5] ; find -name '*.log'./file2.log./file1.log[bash-4.1.5] ; find -name '*.log' | xargs rm[bash-4.1.5] ; find -name '*.log' find+xargs 真的很强大. 然而: 123456789101112[bash-4.1.5] ; ls -ltotal 0-rw-r--r-- 1 root root 0 2010-08-02 18:12 file 1.log-rw-r--r-- 1 root root 0 2010-08-02 18:12 file 2.log[bash-4.1.5] ; find -name '*.log'./file 1.log./file 2.log[bash-4.1.5] ; find -name '*.log' | xargs rmrm: cannot remove `./file': No such file or directoryrm: cannot remove `1.log': No such file or directoryrm: cannot remove `./file': No such file or directoryrm: cannot remove `2.log': No such file or directory 原因其实很简单, xargs 默认是以空白字符 (空格, TAB, 换行符) 来分割记录的, 因此文件名 ./file 1.log 被解释成了两个记录 ./file 和 1.log, 不幸的是 rm 找不到这两个文件. 为了解决此类问题, 聪明的人想出了一个办法, 让 find 在打印出一个文件名之后接着输出一个 NULL 字符 (‘\0’) 而不是换行符, 然后再告诉 xargs 也用 NULL 字符来作为记录的分隔符. 这就是 find 的 -print0 和 xargs 的 -0 的来历吧. 1234567891011[bash-4.1.5] ; ls -ltotal 0-rw-r--r-- 1 root root 0 2010-08-02 18:12 file 1.log-rw-r--r-- 1 root root 0 2010-08-02 18:12 file 2.log[bash-4.1.5] ; find -name '*.log' -print0 | hd 0 1 2 3 4 5 6 7 8 9 A B C D E F |0123456789ABCDEF|--------+--+--+--+--+---+--+--+--+---+--+--+--+---+--+--+--+--+----------------|00000000: 2e 2f 66 69 6c 65 20 31 2e 6c 6f 67 00 2e 2f 66 |./file 1.log../f|00000010: 69 6c 65 20 32 2e 6c 6f 67 00 |ile 2.log. |[bash-4.1.5] ; find -name '*.log' -print0 | xargs -0 rm[bash-4.1.5] ; find -name '*.log' 你可能要问了, 为什么要选 ‘\0’ 而不是其他字符做分隔符呢? 这个也容易理解: 一般的编程语言中都用 ‘\0’ 来作为字符串的结束标志, 文件的路径名中不可能包含 ‘\0’ 字符. find|xargs实例查找当前目录大文件并按大小排序：1234find . -type f -size +800Mfind . -type f -size +800M -print0 | xargs -0 ls -lfind . -type f -size +800M -print0 | xargs -0 du -hfind . -type f -size +800M -print0 | xargs -0 du -h | sort -nr 查找Linux下大目录：1234du -h --max-depth=1du -h --max-depth=2 | sort -ndu -hm --max-depth=2 | sort -ndu -hm --max-depth=2 | sort -nr | head -12 删除以html结尾的10天前的文件，包括带空格的文件：12find /usr/local/backups -name &quot;*.html&quot; -mtime +10 -print0 |xargs -0 rm -rfvfind /usr/local/backups -mtime +10 -name &quot;*.html&quot; -exec rm -rf &#123;&#125; \; 当前目录下文件从大到小排序（包括隐藏文件），文件名不为”.”：12find . -maxdepth 1 ! -name &quot;.&quot; -print0 | xargs -0 du -b | sort -nr | head -10 | nlnl：可以为输出列加上编号,与cat -n相似，但空行不编号 以下功能同上，但不包括隐藏文件：1for file in *; do du -b "$file"; done|sort -nr|head -10|nl xargs结合sed替换：1find . -name "*.txt" -print0 | xargs -0 sed -i 's/aaa/bbb/g' xargs结合grep：1find . -name '*.txt' -type f -print0 |xargs -0 grep -n 'aaa' #“-n”输出行号 用rm删除太多的文件时候，可能得到一个错误信息：/bin/rm Argument list too long. 用xargs去避免这个问题(xargs -0将\0作为定界符)：1find . -type f -name "*.log" -print0 | xargs -0 rm -f]]></content>
      <categories>
        <category>Linux命令</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单点登录解决方案]]></title>
    <url>%2F2017%2F12%2F05%2Fsso-arch%2F</url>
    <content type="text"><![CDATA[什么是SSOWeb应用系统的演化总是从简单到复杂，从单功能到多功能模块再到多子系统方向发展。当前的大中型Web互联网应用基本都是多系统组成的应用群，由多个web系统协同为用户提供服务。多系统应用群，必然意味着各系统间既相对独立，又保持着某种联系。独立，意味着给用户提供一个相对完整的功能服务，比如C2C商城，比如B2C商城。联系，意味着从用户角度看，不管企业提供的服务如何多样化、系列化，在用户看来，仍旧是一个整体，用户体验不能受到影响。譬如用户的账号管理，用户应该有一个统一账号，不应该让用户在每个子系统分别注册、分别登录、再分别登出。系统的复杂性不应该让用户承担。登录用户使用系统服务，可以看做是一次用户会话过程。在单Web应用中，用户登录、登录状态判断、用户登出等操作，已有很常规的解决方案实现。在多系统应用群中，这个问题就变得有些复杂，以前本不是问题的问题，现在可能就变成了一个重大技术问题。我们要用技术手段，屏蔽系统底层本身的技术复杂性，给用户提供自然超爽的用户体验。这就是的单点登录问题，即SSO(Single Sign On)。我们这里主要讨论Web系统，应该叫Web SSO。 实际案例例如阿里系统种类繁多，典型系统应用www.taobao.com 淘宝应用、www.tmall.com 天猫应用、www.alitrip.com 阿里旅游。这些应用，当用户访问时，都需要登录。当我在一个应用如淘宝上登录后，再访问阿里旅游、天猫等其它系统，我们发现，系统都显示已登录状态。当在任意一系统退出登录后，再刷新访问其它系统，均已显示登出状态。可以看出，阿里实现了SSO。实际上，几乎所有提供复杂服务的互联网公司，都实现了SSO，如阿里、百度、新浪、网易、腾讯、58…SSO问题，是大中型Web应用经常碰到的问题，是Java架构师需要掌握的必备技能之一，中高级以上Web工程师都应对它有个了解。 SSO技术难点SSO有啥技术难点？为什么我们不能像解决单Web应用系统登录那样自然解决？为说清楚这一问题，我们得先了解下单应用系统下，用户登录的解决方案。 单应用系统登录我们讨论的应用是Web应用，大家知道，对于Web应用，系统是Browser/Server架构，Browser和Server之间的通信协议是HTTP协议。HTTP是一个无状态协议。即对服务器来说，每次收到的浏览器HTTP请求都是单一独立的，服务器并不考虑两次HTTP请求是否来自同一会话，即HTTP协议是非连接会话状态协议。对于Web应用登录，意味着登录成功后的后续访问，可以看做是登录用户和服务端的一次会话交互过程，直到用户登出结束会话。如何在非连接会话协议之上，实现这种会话的管理？ 我们需要额外的手段。通常有两种做法，一种是通过使用HTTP请求参数传递，这种方式对应用侵入性较大，一般不使用。另一种方式就是通过cookie。cookie是HTTP提供的一种机制，cookie代表一小撮数据。服务端通过HTTP响应创建好cookie后，浏览器会接收下来，下次请求会自动携带上返回给服务端。利用这个机制，我们可以实现应用层的登录会话状态管理。例如我们可以把登录状态信息保存在cookie中，这是客户端保存方式。由于会话信息在客户端，需要维护其安全性、需要加密保存、携带量会变大，这样会影响http的处理效率，同时cookie的数据携带量也有一定的限制。比较好的方式是服务端保存，cookie只保存会话信息的句柄。即在登录成功后，服务端可以创建一个唯一登录会话，并把会话标识ID通过cookie返回给浏览器，浏览器下次访问时会自动带上这个ID，服务端根据ID即可判断是此会话中的请求，从而判断出是该用户，这种操作直到登出销毁会话为止。令人高兴的是，我们使用的Web应用服务器一般都会提供这种会话基础服务，如Tomcat的Session机制。也就是说，应用开发人员不必利用Cookie亲自代码实现会话的创建、维护和销毁等整个生命周期管理，这些内容服务器Session已经提供好了，我们只需正确使用即可。当然，为了灵活性和效率，开发人员也可直接使用cookie实现自己的这种会话管理。对于Cookie，处于安全性考虑，它有一个作用域问题，这个作用域由属性Domain和Path共同决定的。也就是说，如果浏览器发送的请求不在此Cookie的作用域范围内，请求是不会带上此Cookie的。Path是访问路径，我们可以定义/根路径让其作用所有路径，Domain就不一样了。我们不能定义顶级域名如.com，让此Cookie对于所有的com网站都起作用，最大范围我们只能定义到二级域名如.taobao.com，而通常，企业的应用群可能包含有多个二级域名，如taobao.com、tmail.com、alitrip.com等等。这时，解决单系统会话问题的Cookie机制不起作用了，多系统不能共享同一会话，这就是问题的所在！ 当然，有的同学会说：我把所有的应用统一用三级域名来表示，如a.taobao.com、b.taobao.com、c.taobao.com或干脆用路径来区分不同的应用如www.taobao.com\a、www.taobao.com\b、www.taobao.com\c，这样cookie不就可以共享了么？事实是成立的，但现实应用中，多域名策略是普遍存在的，也有商业角度的考虑，这些我们必须要面对。退一步讲，即使cookie可以共享了，服务端如何识别处理这个会话？这时，我们是不能直接使用服务器所提供的Session机制的，Session是在单一应用范围内，共享Session需要特殊处理。更复杂的情况是，通常这些子系统可能是异构的，session实现机制并不相同，如有的是Java系统，有的是PHP系统。共享Session对原系统入侵性很大。至此，SSO技术问题这里讲清楚了。那我们有没有更好的通用解决方案？答案肯定是有的，但比较复杂，这也是我们专题讨论的理由。总体来说，我们需要一个中央认证服务器，来统一集中处理各子系统的登录请求。 SSO实现基本思路单Web应用登录，主要涉及到认证（用户名密码）、授权（权限定义）、会话建立（Cookie&amp;Session）、取消会话（删除Session）等几个关键环节。推广到多系统，每个系统也会涉及到认证、授权、会话建立取消等工作。那我们能不能把每个系统的认证工作抽象出来，放到单独的服务应用中取处理，是不是就能解决单点登录问题？ 思考方向是正确的，我们把这个统一处理认证服务的应用叫认证中心。当用户访问子系统需要登录时，我们把它引到认证中心，让用户到认证中心去登录认证，认证通过后返回并告知系统用户已登录。当用户再访问另一系统应用时，我们同样引导到认证中心，发现已经登录过，即返回并告知该用户已登录 三大关键问题登录信息传递问题应用系统将登录请求转给认证中心，这个很好解决，我们一个HTTP重定向即可实现。现在的问题是，用户在认证中心登录后，认证中心如何将消息转回给该系统？这是在单web系统中不存在的问题。我们知道HTTP协议传递消息只能通过请求参数方式或cookie方式，cookie跨域问题不能解决，我们只能通过URL请求参数。我们可以将认证通过消息做成一个令牌(token)再利用HTTP重定向传递给应用系统。但现在的关键是：该系统如何判断这个令牌的真伪？如果判断这个令牌确实是由认证中心发出的，且是有效的？我们还需要应用系统和认证中心之间再来个直接通信，来验证这个令牌确实是认证中心发出的，且是有效的。由于应用系统和认证中心是属于服务端之间的通信，不经过用户浏览器，相对是安全的。 用户首次登录时流程如下： 用户浏览器访问系统A需登录受限资源。 系统A发现该请求需要登录，将请求重定向到认证中心，进行登录。 认证中心呈现登录页面，用户登录，登录成功后，认证中心重定向请求到系统A，并附上认证通过令牌。 系统A与认证中心通信，验证令牌有效,证明用户已登录。 系统A将受限资源返给用户。 已登录用户首次访问应用群中系统B时： 浏览器访问另一应用B需登录受限资源。 系统B发现该请求需要登录，将请求重定向到认证中心，进行登录。 认证中心发现已经登录，即重定向请求响应到系统B，附带上认证令牌。 系统B与认证中心通信，验证令牌有效,证明用户已登录。 系统B将受限资源返回给客户端。 登录状态判断问题用户到认证中心登录后，用户和认证中心之间建立起了会话，我们把这个会话称为全局会话。当用户后续访问系统应用时，我们不可能每次应用请求都到认证中心去判定是否登录，这样效率非常低下，这也是单Web应用不需要考虑的。我们可以在系统应用和用户浏览器之间建立起局部会话，局部会话保持了客户端与该系统应用的登录状态，局部会话依附于全局会话存在，全局会话消失，局部会话必须消失。用户访问应用时，首先判断局部会话是否存在，如存在，即认为是登录状态，无需再到认证中心去判断。如不存在，就重定向到认证中心判断全局会话是否存在，如存在，按1提到的方式通知该应用，该应用与客户端就建立起它们之间局部会话，下次请求该应用，就不去认证中心验证了。 登出问题用户在一个系统登出了，访问其它子系统，也应该是登出状态。要想做到这一点，应用除结束本地局部会话外，还应该通知认证中心该用户登出。认证中心接到登出通知，即可结束全局会话，同时需要通知所有已建立局部会话的子系统，将它们的局部会话销毁。这样，用户访问其它应用时，都显示已登出状态。整个登出流程如下： 客户端向应用A发送登出Logout请求。 应用A取消本地会话，同时通知认证中心，用户已登出。 应用A返回客户端登出请求。 认证中心通知所有用户登录访问的应用，用户已登出。 实现SSO实现分成两大部分，一个是SSO Server，代表认证中心，另一个是SSO Client，代表使用SSO系统应用的登录登出组件。 登录令牌token实现前面我们讨论了，系统把用户重定向导向认证中心并登录后，认证中心要把登录成功信息通过令牌方式告诉给应用系统。认证中心会记录下来自某个应用系统的某个用户本次通过了认证中心的认证所涉及的基本信息，并生成一个登录令牌token，认证中心需要通过URL参数的形式把token传递回应用系统，由于经过客户端浏览器，故令牌token的安全性很重要。因此令牌token的实现要满足三个条件： 首先，token具有唯一性，它代表着来自某应用系统用户的一次成功登录。我们可以利用java util包工具直接生成一个32位唯一字符串来实现。1String token = UUID.randomUUID().toString(); 同时，我们定义一个javabean， TokenInfo 来承载token所表示的具体内容，即某个应用系统来的某个用户本次通过了认证中心123456public class TokenInfo &#123; private int userId; //用户唯一标识ID private String username; //用户登录名 private String ssoClient; //来自登录请求的某应用系统标识 private String globalId; //本次登录成功的全局会话sessionId ... &#125; token和tokenInfo形成了一个&lt;key,value&gt;形式的键值对，后续应用系统向认证中心验证token时还会用到。其次，token存在的有效期间不能过长，这是出于安全的角度，例如token生存最大时长为60秒。我们可以直接利用redis特性来实现这一功能。redis本质就是&lt;key,value&gt;键值对形式的内存数据库，并且这个键值对可以设置有效时长。第三，token只能使用一次，用完即作废，不能重复使用。这也是保证系统安全性。我们可以定义一个TokenUtil工具类，来实现&lt;token,tokenInfo&gt;键值对在redis中的操作，主要接口如下：123456789public class TokenUtil &#123; ... // 存储临时令牌到redis中，存活期60秒 public static void setToken(String tokenId, TokenInfo tokenInfo)&#123; ... &#125; //根据token键取TokenInfo public static TokenInfo getToken(String tokenId)&#123; ... &#125; //删除某个 token键值 public static void delToken(String tokenId)&#123; ... &#125; &#125; 全局会话和本地会话的实现用户登录成功后，在浏览器用户和认证中心之间会建立全局会话，浏览器用户与访问的应用系统之间，会建立本地局部会话。为简便可以使用web应用服务器(如tomcat)提供的session功能来直接实现。这里需要注意的是，我们需要根据会话ID(即sessionId)能访问到这个session。因为根据前面登出流程说明，认证中心的登出请求不是直接来自连接的浏览器用户，可能来自某应用系统。认证中心也会通知注册的系统应用进行登出。这些请求，都是系统之间的交互，不经过用户浏览器。系统要有根据sessionId访问session的能力。同时，在认证中心中，还需要维护全局会话ID和已登录系统本地局部会话ID的关系，以便认证中心能够通知已登录的系统进行登出处理。为了安全，目前的web应用服务器，如tomcat，是不提供根据sessionId访问session的能力的，那是容器级范围内的能力。我们需要在自己的应用中，自己维护一个sessionId和session直接的对应关系，我们把它放到一个Map中，方便需要时根据sessionId找到对应的session。同时，我们借助web容器提供的session事件监听能力，程序来维护这种对应关系。认证中心涉及到两个类，GlobalSessions和GlobalSessionListener，相关代码如下：12345678910111213141516171819202122232425public class GlobalSessions &#123; //存放所有全局会话 private static Map&lt;String, HttpSession&gt; sessions = new HashMap&lt;String,HttpSession&gt;(); public static void addSession(String sessionId, HttpSession session) &#123; sessions.put(sessionId, session); &#125; public static void delSession(String sessionId) &#123; sessions.remove(sessionId); &#125; //根据id得到session public static HttpSession getSession(String sessionId) &#123; return sessions.get(sessionId); &#125;&#125; public class GlobalSessionListener implements HttpSessionListener &#123; public void sessionCreated(HttpSessionEvent httpSessionEvent) &#123; GlobalSessions.addSession( httpSessionEvent.getSession().getId(), httpSessionEvent.getSession()); &#125; public void sessionDestroyed(HttpSessionEvent httpSessionEvent) &#123; GlobalSessions.delSession(httpSessionEvent.getSession().getId()); &#125; &#125; SSO Client对应的是LocalSessions和LocalSessionListener,实现方式同上。 应用系统和认证中心之间的通信根据SSO实现流程，应用系统和认证中心之间需要直接通信。如应用系统需要向认证中心验证令牌token的真伪，应用系统通知认证中心登出，认证中心通知所有已注册应用系统登出等。这是Server之间的通信，如何实现呢？我们可以使用HTTP进行通信，返回的消息应答格式可采用JSON格式。Java的net包，提供了http访问服务器的能力。这里，我们使用apache提供的一个更强大的开源框架，httpclient，来实现应用系统和认证中心之间的直接通信。JSON和JavaBean之间的转换，目前常用的有两个工具包，一个是json-lib，还有一个是Jackson，Jackson效率较高，依赖包少，社区活跃度大，这里我们使用Jackson这个工具包。如应用系统向认证中心发送token验证请求的代码片段如下： 123456789101112131415161718192021222324252627282930//向认证中心发送验证token请求 String verifyURL = "http://" + server + PropertiesConfigUtil.getProperty("sso.server.verify"); HttpClient httpClient = new DefaultHttpClient(); //serverName作为本应用标识 HttpGet httpGet = new HttpGet(verifyURL + "?token=" + token + "&amp;localId=" + request.getSession().getId()); try&#123; HttpResponse httpResponse = httpClient.execute(httpGet); int statusCode = httpResponse.getStatusLine().getStatusCode(); if (statusCode == HttpStatus.SC_OK) &#123; String result = EntityUtils.toString(httpResponse.getEntity(), "utf-8"); //解析json数据 ObjectMapper objectMapper = new ObjectMapper(); VerifyBean verifyResult = objectMapper.readValue(result, VerifyBean.class); //验证通过,应用返回浏览器需要验证的页面 if(verifyResult.getRet().equals("0")) &#123; Auth auth = new Auth(); auth.setUserId(verifyResult.getUserId()); auth.setUsername(verifyResult.getUsername()); auth.setGlobalId(verifyResult.getGlobalId()); request.getSession().setAttribute("auth", auth); //建立本地会话 return "redirect:http://" + returnURL; &#125; &#125; &#125; catch (Exception e) &#123; return "redirect:" + loginURL; &#125; SSO Server接口核心实现细节讨论清楚了，我们就可以根据登录登出操作流程，定义SSO Server和SSO Client所提供的接口。SSO Server认证中心包含4个重要接口，分别如下： 接口一： /page/login。此接口主要接受来自应用系统的认证请求，此时，returnURL参数需加上，用以向认证中心标识是哪个应用系统，以及返回该应用的URL。如用户没有登录，应用中心向浏览器用户显示登录页面。如已登录，则产生临时令牌token，并重定向回该系统。上面登录时序交互图中的2和此接口有关。当然，该接口也同时接受用户直接向认证中心登录，此时没有returnURL参数，认证中心直接返回登录页面。123接口名称：/page/login入参： returnURL (系统URL，可选)返回： 1.显示登录页面；2.产生临时认证token并重定向回系统； 接口二： /auth/login。处理浏览器用户登录认证请求。如带有returnURL参数，认证通过后，将产生临时认证令牌token，并携带此token重定向回系统。如没有带returnURL参数，说明用户是直接从认证中心发起的登录请求，认证通过后，返回认证中心首页提示用户已登录。上面登录时序交互图中的3和此接口有关。123接口名称：/auth/login入参： username[*用户名]、password[*密码]、returnURL返回： 1.产生临时认证token并重定向回系统；2.返回认证中心首页提示登录成功； 接口三： /auth/verify。认证应用系统来的token是否有效，如有效，应用系统向认证中心注册，同时认证中心会返回该应用系统登录用户的相关信息，如ID,username等。上面登录时序交互图中的4和此接口有关。12345678910接口名称：/auth/verify入参： token、localid返回： JSON格式消息 &#123; ret: 返回结果字符串，0表示成功； msg: 返回结果文字说明字符串； userid: 用户ID; username: 用户登录名; globalid: 全局会话ID,登出时使用; &#125; 接口四： /auth/logout。登出接口处理两种情况，一是直接从认证中心登出，一是来自应用重定向的登出请求。这个根据gId来区分，无gId参数说明直接从认证中心注销，有，说明从应用中来。接口首先取消当前全局登录会话，其次根据注册的已登录应用，通知它们进行登出操作。123接口名称：/auth/logout入参： gid[全局会话id,可选]返回： 1.返回OK; 2.返回认证中心首页; SSO Client接口接口一： /auth/check。接收来自认证中心携带临时令牌token的重定向，向认证中心/auth/verify接口去验证此token的有效性，如有效，即建立本地会话，根据returnURL返回浏览器用户的实际请求。如验证失败，再重定向到认证中心登录页面。123接口名称：/auth/check入参： token[*登录token]返回： 成功重定向returnURL，失败重定向到登录页面; 接口二： /auth/logout。处理两种情况，一种是浏览器向本应用接口发出的直接登出请求，应用会消除本地会话，调用认证服务器/auth/logout接口，通知认证中心删除全局会话和其它已登录应用的本地会话。 如果是从认证中心来的登出请求，此时带有localId参数，接口实现会直接删除本地会话，返回字符串”ok”。123接口名称：/auth/logout入参： localid[本地会话id,可选]返回： 1.首页; 2.'ok'字符串; CASCAS是中央认证服务Central Authentication Service的简称。最初由耶鲁大学的Shawn Bayern 开发，后由Jasig社区维护，经过十多年发展，目前已成为影响最大、广泛使用的、基于Java实现的、开源SSO解决方案。2012年，Jasig和另一个有影响的组织Sakai Foundation合并，组成Apereo。Apereo是一个由高等学术教育机构发起组织的联盟，旨在为学术教育机构提供高质量软件，当然很多软件也被大量应用于商业环境，譬如CAS。目前CAS由Apereo社区维护。CAS的官方网址是： https://www.apereo.org/projects/cas工程代码网址：https://github.com/Jasig/cas CAS也提供了一个认证中心，叫CAS Server，参与登录的应用系统都会引导到CAS Server进行登录。各应用系统与CAS Server交互通信的登录组件叫CAS Client。如CAS Client，已经提供了包括Java、.net、php、ruby、perl等多种语言的实现，非常适合异构系统的单点登录使用场景。再比如认证方式，除了常见的基于数据库认证，还提供LDAP使用场景,同时支持各种常见认证协议，如spnego、OpenId、X509等等。对于全局会话，CAS基于Cookie使用了自己的实现方式，而服务端的会话存储，除了缺省基于内存模式，还提供了基于ehcache、memcached等多种实现，同时提供了灵活接口便于自己定制扩展，这非常适合某些高可用性、高性能的应用场景。因此，在一般场景下，我们不需要重新发明轮子，直接在成熟技术框架基础上开发使用即可。这也是CAS在很多互联网和企业应用中广泛使用的原因。当然，对于某些场景，如安全性因素、更特殊更高效的应用场景，在技术实力许可的情况下，通常都自己实现SSO。]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[开发者的网络安全技术]]></title>
    <url>%2F2017%2F11%2F29%2Fnetsafe-technology%2F</url>
    <content type="text"><![CDATA[本文主要对软件开发者相关的系统安全问题进行总结。 攻击类型XSSCSRF产生原理 想要攻击成功，这三步缺一不可。 第一，登录受害者网站。如果受害者网站是基于 cookie 的用户验证机制，那么当用户登录成功后，浏览器就会保存一份服务端的 SESSIONID。 第二，这时候在同一个浏览器打开攻击者网站，虽然说它无法获取 SESSIONID 是什么（因为设置了 http only 的 cookie 是无法被 JavaScript 获取的），但是从浏览器向受害者网站发出的任何请求中，都会携带它的 cookie，无论是从哪个网站发出。 第三，利用这个原理，在攻击者网站发出一个请求，命令受害者网站进行一些敏感操作。由于此时发出的请求是处于 session 中的，所以只要该用户有权限，那么任何请求都会被执行。 比如，打开优酷，并登录。再打开攻击者网站，它里面有个 &lt;img&gt; 标签是这样的： &lt;img src="http://api.youku.com/follow/123" /&gt; 这个 api 只是个例子，具体的 url 和参数都可以通过浏览器的开发者工具（Network 功能）事先确定。假如它的作用是让该登录的用户关注由 123 确定的一个节目或者用户，那么通过 CSRF 攻击，这个节目的关注量就会不断上升。 解释两点。第一，为什么举这个例子，而不是银行这种和金钱有关的操作？很简单，因为它容易猜。对于攻击者来说，没有什么是一定能成功的，比如 SQL 注入，攻击者他不知道某网站的数据库是怎么设计的，但是他一般会通过个人经验去尝试，比如很多网站把用户的主键设置为 user_id，或 sys_id 等。 银行的操作往往经过多重确认，比如图形验证码、手机验证码等，光靠 CSRF 完成一次攻击基本上是天方夜谭。但其他类型的网站往往不会刻意去防范这些问题。虽然金钱上的利益很难得到，但 CSRF 能办到的事情还是很多，比如利用别人发虚假微博、加好友等，这些都能对攻击者产生利益。 第二，如何确保用户打开优酷之后，又打开攻击者网站？做不到。否则任何人打开优酷之后，都会莫名其妙地去关注某个节目了。但是你要知道，这个攻击成本仅仅是一条 API 调用而已，它在哪里都能出现，你从任何地方下载一张图片，让你请求这个地址，看也不看就点确定，请求不就发出去了吗？ 防范手段对于如何防范 CSRF，一般有三种手段。 判断请求头Referer这个字段记录的是请求的来源。比如 http://www.example.com 上调用了百度的接口 http://api.map.baidu.com/service 那么在百度的服务端，就可以通过 Referer 判断这个请求是来自哪里。 在实际应用中，这些跟业务逻辑无关的操作往往会放在拦截器中（或者说过滤器，不同技术使用的名词可能不同）。意思是说，在进入到业务逻辑之前，就应该要根据 Referer 的值来决定这个请求能不能处理。 在 Java Servlet 中可以用 Filter（古老的技术）；用 Spring 的话可以建拦截器；在 Express 中是叫中间件，通过 request.get(‘referer’) 来取得这个值。每种技术它走的流程其实都一样。 但要注意的是，Referer 是浏览器设置的，在浏览器兼容性大不相同的时代中，如果存在某种浏览器允许用户修改这个值，那么 CSRF 漏洞依然存在。 在请求参数中加入 csrf token讨论 GET 和 POST 两种请求，对于 GET，其实也没什么需要防范的。为什么？因为 GET 在“约定”当中，被认为是查询操作，查询的意思就是，你查一次，查两次，无数次，结果都不会改变（用户得到的数据可能会变），这不会对数据库造成任何影响，所以不需要加其他额外的参数。 所以这里要提醒各位的是，尽量遵从这些约定，不要在 GET 请求中出现 /delete, /update, /edit 这种单词。把“写”操作放到 POST 中。 对于 POST，服务端在创建表单的时候可以加一个隐藏字段，也是通过某种加密算法得到的。在处理请求时，验证这个字段是否合法，如果合法就继续处理，否则就认为是恶意操作。 &lt;form method="post" action="/delete"&gt; &lt;!-- 其他字段 --&gt; &lt;input type="hidden" name="csrftoken" value="由服务端生成"/&gt; &lt;/form&gt; 这个 html 片段由服务端生成，比如 JSP，PHP 等，对于 Node.js 的话可以是 Jade 。 这的确是一个很好的防范措施，再增加一些处理的话，还能防止表单重复提交。 可是对于一些新兴网站，很多都采用了“单页”的设计，或者退一步，无论是不是单页，它的 HTML 可能是由 JavaScript 拼接而成，并且表单也都是异步提交。所以这个办法有它的应用场景，也有局限性。 新增 HTTP Header思想是，将 token 放在请求头中，服务端可以像获取 Referer 一样获取这个请求头，不同的是，这个 token 是由服务端生成的，所以攻击者他没办法猜。这篇文章的另一个重点——JWT——就是基于这个方式。抛开 JWT 不谈，它的工作原理是这样的: 解释一下这四个请求，类型都是 POST 。 1.通过 /login 接口，用户登录，服务端传回一个 access_token，前端把它保存起来，可以是内存当中，如果你希望用来模拟 session 的话。也可以保存到 localStorage 中，这样可以实现自动登录。2.调用 /delete 接口，参数是某样商品的 id。仔细看，在这个请求中，多了一个名为 Authoriaztion 的 header，它的值是之前从服务端传回来的 access_token，在前面加了一个“Bearer”（这是和服务端的约定，约定就是说，说好了加就一起加，不加就都不加……）3.调用 /logout 接口，同样把 access_token 加在 header 中传过去。成功之后，服务端和前端都会把这个 token 置为失效，或直接删除。4.再调用 /delete 接口，由于此时已经没有 access_token 了，所以服务端判断该请求没权限，返回 401 。 各位有没有发现，从头至尾，整个过程没有涉及 cookie，所以 CSRF 是不可能发生的！ 重放攻击]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HEXO&Jekyllrb Tutorial]]></title>
    <url>%2F2017%2F11%2F20%2Fhexo-jekyllrb-tutorial%2F</url>
    <content type="text"><![CDATA[HEXO是一个静态BLOG APPS。 Hexo安装配置安装1cnpm install hexo-cli -g 启动1hexo server 配置语言配置1language: zh-Hans 部署12hexo cleanhexo generate NEXT主题12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 克隆/下载 完成后，打开 站点配置文件，找到theme字段，并将其值更改为next。 这里注意区分两个配置文件：站点配置文件：是你的 hexo 博客目录下面的 _config.yml 文件。主题配置文件：是 themes/next 目录下的 _config.yml 文件。 打开侧边栏12sidebar: display: always 高级功能搜索12cnpm install hexo-generator-search --savecnpm install hexo-generator-searchdb --save 在站点配置中加入：12345search: path: search.xml field: post format: html limit: 10000 在NEXT主题配置文件中开启：12local_search: enable: true 关于我页面1hexo new page about 在NEXT主题中开启关于我页面即可 写作阅读全文在适当的位置加入如下标签即可：1&lt;!--more--&gt; 创建文章12hexo new [layout] &lt;title&gt;hexo new MyPage 插入图片修改_config.yml配置文件post_asset_folder项为true。创建文章使用命令如下：1hexo new 'article title' 使用完命令之后，在source/_post文件夹里面就会出现一个“article title.md”的文件和一个“article title”的文件夹。 Jekyllrb安装配置Jekyllrb依赖Ruby，需要安装Ruby环境 安装Minimal MistakesMinimal Mistakes（https://mademistakes.com/work/minimal-mistakes-jekyll-theme/）是一个Jekyllrb主题]]></content>
      <categories>
        <category>小技巧</category>
      </categories>
  </entry>
</search>
